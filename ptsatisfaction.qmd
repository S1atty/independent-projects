---
title: "Forecasting Patient Satisfaction for Emergency Department Encounters"
author: "Tomi Akisanya"
output: html_document
editor_options: 
settings:
  chunk_output_type: console
---

```{r, echo = TRUE, eval = TRUE, include=FALSE}
library(tidyverse)
# library(lessR)
library(ggplot2)
library(car)
library(readxl)
library(magrittr)
library(skimr)
library(dplyr)
```

![](images/ptsatisfaction_image.jpeg){fig-align="right" width="862"}

## Abstract

Patient satisfaction can be an indication of quality care and aid in predicting health outcomes and patient retention. Although subjective, it is crucial for healthcare administrators to understand and meet patients' expectations, translate it to patient-oriented care within delivery models, and improve population health as a result. This project investigates features associated with higher patient satisfaction scores for ER encounters and developing models to predict them.

## Data

This data set is a patient satisfaction survey administered to patients discharged from ten different entities at NMH over the span of two years. Respondents were discharged from six emergency units. Surveys were delivered either on paper or electronically following their encounter. The patient satisfaction surveys collected, range from discharges starting March of 2016 through January of 2018. Responses were given in either English or Spanish. Five types of payers were used for the encounters and satisfaction scores per response were given on a scale of one (very dissatisfied) to five (very satisfied). No other information on how the data was collected is available.



```{r echo=FALSE,eval=TRUE, message=FALSE, warning=FALSE, include = FALSE}
### Patient Demographics

MHSA_NM_DATA_PTEXP_ER <- read_excel("Patient Satisfaction Data/MHSA_NM_DATA_PTEXP_ER.xlsx") #11688 obs of 45 questions
MHSA_NM_DATA_PTEXP_ER_DICTIONARY <- read_excel("Patient Satisfaction Data/MHSA Datasets Dictionary (2).xlsx")
mydata <- data.frame(MHSA_NM_DATA_PTEXP_ER) 
mydata %<>% mutate(er_admit_time = date(er_admit_time), 
                   er_disch_time = date(er_disch_time),
                   payor = case_when(payor == 'MEDICARE' ~ 'Medicare', .default = payor))

# check initial demographic structures

for (i in 1:ncol(mydata)){
  if (is.character(mydata[,i]) && i != 5 && i != 6 && i != 12){
   print(table(mydata[,i]))
  }
}


# sum(diff(mydata$disdate), na.rm = TRUE) # date of discharge
# sum(diff(mydata$recdate), na.rm = TRUE) # date survey sent
# 
# range(mydata$disdate) #DATE SURVEY WAS ADMINISTERED
# range(mydata$recdate) #DATE OF DISCHARGE FROM THE HOSPITAL SURVEY FOR THE ENCOUNTER SURVEYED
# range(mydata$er_admit_time, na.rm = TRUE) # ER arrival time stamp
# range(mydata$er_disch_time, na.rm = TRUE) # ER discharge time stamp


mydata <- mydata[mydata$disdate!="1900-01-01" & mydata$survey_id!=1418129390,] #11687

```

## Population of Interest

The population of interest is the patient population for emergency encounters at NM with disproportionate social determinants of health.

## Exclusion Criteria

A waterfall approach was used to define the population of interest. Social determinants, which include environmental and non-health related factors such as socio-economic status, can account for 30-55% of health outcomes[^1]. Payer type was used as a proxy to define subsets of the population with disproportionate social determinants, with financial payers like Medicaid typically insuring low-income individuals. Self-pay and Medicaid payers were also included in the drop-down conditions, as this subset tends to misuse the ER for non-urgent care, serving as a viable proxy for lower health literacy.[^2]. Patterns of ER misuse are also more likely to be younger and of non-hispanic black race, however, the only information available an race were responses indicating white or non-white. While sub-setting race to only non-whites would align more with the population of interest, this information was retained to maintain the basis of interpretation and prevent over specification of models in the event covariates are used. The age at which we thought individuals could best understand and articulate aspects of the emergency department setting was 16 years old. Records younger than that were omitted.

[^1]: Williams JN, Drenkard C, Lim SS. The impact of social determinants of health on the presentation, management and outcomes of systemic lupus erythematosus. Rheumatology (Oxford). 2023 Mar 29;62(Suppl 1):i10-i14. doi: 10.1093/rheumatology/keac613. PMID: 36987604; PMCID: PMC10050938.

[^2]: Naouri D, Ranchon G, Vuagnat A, Schmidt J, El Khoury C, Yordanov Y; French Society of Emergency Medicine. Factors associated with inappropriate use of emergency departments: findings from a cross-sectional national study in France. BMJ Qual Saf. 2020 Jun;29(6):449-464. doi: 10.1136/bmjqs-2019-009396. Epub 2019 Oct 30. PMID: 31666304; PMCID: PMC7323738.

```{r}
waterfall <- mydata[mydata$disdate!="1900-01-01" & mydata$survey_id!=1418129390,] #erroneous record; clearly incorrect entry 
waterfall %<>% filter(age >= 16, 
                      payor %in% c("Medicare","Medicaid","Self-Pay")) #4603


waterfall %<>%
  filter(!is.na(er_disch_time)) %>%
  filter(!is.na(er_admit_time))

for (i in seq_len(nrow(waterfall))){
  if(sum(is.na(waterfall[i,]))>8){
    waterfall <- waterfall[-i,]
  }
} #4101

for (i in seq_along(ncol((waterfall)))){
  if(sum(is.na(waterfall[,i]))/4101>.25){
  #  print(names(mydata[,i]))
    waterfall <- waterfall[,-i]
  }
} 

# } #44 variable removed A87 / D2 / D52  = 4099 obs of 45 var


waterfall %<>%  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

waterfall %<>% mutate(family_friends = (E1+E2+E3)/3,
                      tests = (D4+D65+D3)/3,
                      doctors = (C2+C4+C5+C75)/4,
                      nurses = (B1+B3+B4+B5+B76)/5, 
                      arrival = (A4+A5+A86+C1)/4,
                      personal_issues = (F1+F2+F41)/3,
                      insurance = (A2+A3+A28)/3, 
                      overall = (E1+E2+E3+D4+D65+C1+C2+C4+C5+C75+B1+B3+B4+B5+B76+A4+A5+A86+F1+F2+F41+A2+A3+A28)/25)
# additional variables will most likely remove due to multicollinearity

waterfall %<>% mutate(response_time = recdate - disdate, .before = distrib) # length of time b/w disch and receiving survey

waterfall %<>% filter(response_time>=0) # filter out retroactive response times 

dim(waterfall) # final sample 2875 observations of 51 variables

```

After cleaning records and additional formatting, observations were then cleaned. Survey ID 1418129390 had missing admit and discharge times and the encounter record for the survey was dated 1900-01-01. The implicit erroneous record was removed.

To maintain the integrity of the data set, observations with missing discharge or admit times were removed.

Observations were subset to those over the age of 16 who were Medicare, Medicaid, or Self-Pay users to isolate the patient population. No demographic attributes were missing thereafter.

Respondents were asked to survey their satisfaction across different care settings during their encounter. The Likert scale of possible responses were 1-5.

For the non-demographic survey questions, there were a number of missing responses. Survey respondents that had greater than 30% of answers missing were removed from the drop down conditions. Subsequently, survey questions across the sample that had more than 30% of responses missing were also omitted. The questions removed were:

A87 (Arrival) - Helpfulness of person who first asked you about your condition D2 (Tests) - Courtesy of the person who took your blood D52 (Tests) - Concern shown for your comfort when your blood was drawn

A new variable was created to measure the length of time between discharge date and the date the survey was received. Survey IDs with negative response times were present, implying that they received the survey before being admitted to the ER. These records were concluded as an error and were also omitted.

Survey responses could be grouped into one of the following care setting categories: Arrival, Personal/Insurance Info, Nurses, Doctors, Tests, Family or Friends, Personal Issues.

For any missing responses afterwards, the missing values were replaced with the average satisfaction score for that question.

### Model Validation Setup

```{r, eval = TRUE, echo = TRUE, include = TRUE}

set.seed(555)

# 70/30 test split 

waterfall %<>% mutate(valid = runif(survey_id,0,1))
train <- waterfall %>%
  filter(valid <= .7)
test <- waterfall %>%
  filter(valid > .7)

dim(train)
dim(test)

```

## Methods

A regression model was fit to predict the overall patient satisfaction variable, `F68,` and used to estimate the true population parameters. A 70/30 train:test ratio was used for model validation. Regression assumptions were then validated. For residual normality and increased model accuracy, explanatory variables were transformed before model fitting. The sampling distribution forms a left-skew distribution, in which the sixth root and natural log transformations were applied.

```{r, eval = TRUE, echo = FALSE, include = TRUE, message=FALSE, fig.width = 8, fig.height=8}

d1 <- 
train %>%
  filter(F68 %in% c(1,2,3,4,5)) %>%
  ggplot(aes(x=F68))+
  geom_density()+
  geom_histogram(aes(y=after_stat(count)/sum(after_stat(count))))+
  scale_x_continuous(name = "Overall Satisfaction")+
  scale_y_continuous(name = "Density", limits = c(0,1.5))+
  labs(title = "Overall Rating of Care Received during Visit",
       subtitle = "1) Unscaled, 2) Log, 3) Root")+
  theme_bw()
# lines(density(train$F68))

# psych::describe(train$F68)

d2 <- 
train %>%
  filter(F68 %in% c(1,2,3,4,5)) %>%
  ggplot(aes(x=log(F68)))+
  geom_density()+
  geom_histogram(aes(y=after_stat(count)/sum(after_stat(count))))+
  scale_x_continuous(name = "Overall Satisfaction")+
  scale_y_continuous(name = "Density", limits = c(0,1.5))+
  theme_bw()
# lines(density(log(train$F68)))

# psych::describe(log(train$F68))

# d3 <- 
# train %>%
#   filter(F68 %in% c(1,2,3,4,5)) %>%
#   ggplot(aes(x=sqrt(F68)))+
#   geom_density()+
#   geom_histogram(aes(y=after_stat(count)/sum(after_stat(count))))+
#   scale_x_continuous(name = "Overall Satisfaction")+
#   scale_y_continuous(name = "Density", limits = c(0,1.5))+
#   theme_bw()
# lines(density(sqrt(train$F68)))
# psych::describe(sqrt(train$F68))

d4 <- 
train %>%
  filter(F68 %in% c(1,2,3,4,5)) %>%
  ggplot(aes(x=F68^(1/6)))+
  geom_density()+
  geom_histogram(aes(y=after_stat(count)/sum(after_stat(count))))+
  scale_x_continuous(name = "Overall Satisfaction")+
  scale_y_continuous(name = "Density", limits = c(0,1.5))+
  theme_bw()


gridExtra::grid.arrange(d1,d2,d4, ncol=1, newpage = TRUE)

```

Observing that the root transformation resulted in the greatest shift towards normality, it was applied to all explanatory variables. Additionally, summary variables based on category were calculated to enhance predictive power, specifically by computing the mean averages for each category. Several transformations were performed on the predictor variable F68 to improve the overall coefficient of determination and model fit. Consequently, the final explanatory parameters are based on the root of F68.

**Pearson's product correlation matrix** was used for variable selection; starting with the highest coefficient of determination, a regression model was fit in progressive steps with one or two variables at a time. At each step, a combination of diagnostic tests and empirical thresholds were used to assess goodness-of-fit based on the following criteria:

$$ Δ R^2 = R_f^2 - R_n^2 > .2 $$

Where the additional variable must increase the coefficient of determination by 2% in order for the variable to be retained.

**Turkey's Nonadditive Test** was used to asses presence of interaction. In the context of regression, fitted values squared are computed post-hoc as a quadratic function to test if the interaction term is significantly different from zero, assuming H~0~: ŷ = 0 and a linear function is modeled with H~a~: ŷ ≠ 0 and a non-linear function is modeled:

$$\hat{f_n} = (β_1)\hat{x_1} + (β_2)\hat{x_2}+ (β_3)\hat{x_3}...+(\hat{y_n})^2\hat{x_z}$$

$$\hat{y_n} = (β_1)\hat{x_1} + (β_2)\hat{x_2}...+(β_z)\hat{x_z}$$

where ŷ~f~ are the full predicted values and ŷ~n~ are the nested predicted values, variables were retained if P(F) \< .05.

**Difference in fits** was used to measure the influence of individual observations obtained using an empirical threshold of ${\small √(p)/n}$; where p is the number of parameters and n is the number of observations

**Cook's distance** uses leverage and studentized residuals to measure significance of observation on overall model; threshold of 4/n was considered to be practically influential and was further evaluated

**Wald Statistic**: {\large $β^2/Var(β)$} measures the effect size of individual parameters.

**Variance Inflation Factors** greater than four were reevaluated; greater than seven are removed.

The model yielding the optimal Δ R^2^ and least RMSE was used as the production model. The questions used to guide the final model were:

1.  Does the model capture the true population parameters and relationship? Can the model be used to draw generalizations from our target population?
2.  Is the model an accurate predictor of patient satisfaction?

The results of the diagnostic post-hoc tests were then used to assess the models capability to generalize to the target population and decide if a different model would be more equipped to capture the relationships found in survey responses to overall patient satisfaction in ER encounters for those with low social determinants of health. In practice, it is acknowledged that **no generalizations can accurately be made on our target population** due to the limited information around how the data was collected. It is unknown if the observations were from a random sample, which is sufficient and necessary to make any population conclusions. Any aforementioned hypothesis to the population is for the sake of statistical inference and this endeavor. However, to validate prediction capabilities, a precision grade was used on our test set.

```{r, eval = FALSE, echo = FALSE, include = FALSE}

#original model validation setup 
set.seed(555)

# 70/30 test split 

waterfall %<>% mutate(valid = runif(survey_id,0,1))
train <- waterfall %>%
  filter(valid <= .7)
test <- waterfall %>%
  filter(valid > .7)

dim(train)
dim(test)

```

## Analysis

Pearson's product correlation plot showed that overall satisfaction was strongly correlated with how well patients were informed about delays and other responses based on either doctors' or nurses' care setting.

```{r, eval = TRUE, echo = FALSE}

r_table1 <- tibble(Variable = c("F2", "B3", "B4", "C4", "C2", "B5", "C1"),
                   Correlation = c(.796, .734, .733, .727, .674, .655, .629),
                   Category = c("Personal Issues", rep("Nurses",3), rep("Doctors",3)),
                   Question = c("How well you were kept informed about delays","Nurses' attention to your needs",
                           "Nurses' concern to keep you informed about your treatment","Doctor's concern for your comfort while treating you",
                           "Courtesy of the doctor","Nurses' concern for your privacy",
                           "Waiting time in the treatment area, before you were seen by a doctor"))

knitr::kable(r_table1)

```

```{r, eval = FALSE, echo = FALSE, include = FALSE}

train %>%
  select(where(is.numeric)) %>%
  cor(method = "pearson") %>%
 # corrplot::corrplot() # F2 / nurses / B5 / B3 / B4 / C1 / C2 / C4
  corrplot::corrplot(main = "Correlation Matrix")

```

F2 was therefore used as the initial predictive variable and subsequent variables were added in a 'stepwise' fashion while assessing the change in R^2^ and MSE. Variables within the same category exhibited a high degree of underlying collinearity and summary statistics were calculated for the means of the top two responses or the total responses per group to remediate covariance. This is preferable since 1) information can be easily summarized into one parameter instead of multiple, decreasing the Alkaline Information Criterion while maintaining parsimony and 2) missing responses are a notable characteristic of survey data and limiting the model to individual parameters can lead to inaccuracy for future data sets that have large proportions of missing responses to individual questions.

```{r}

train %<>%
  mutate(across(c(16:46), ~ (.)^(1/6), .names = "{col}_rt"), .before = family_friends) 
            
train %<>%
  mutate(arrival_2 = (A5_rt+C1_rt)/2, doctors_2 = (C4_rt+C75_rt)/2,
         family_friends_2 = (E1_rt+E3_rt)/2, nurses_2 = (B4_rt+B3_rt)/2,
         personal_issues_2 = (F2_rt+F41_rt)/2, test_2 = (D3_rt+D4_rt)/2) #averages post-trans

train %<>%
  mutate(arrival_3 = (A5_rt+C1_rt+A86_rt)/3, doctors_3 = (C4_rt+C75_rt+C5_rt)/3,
         family_friends_3 = (E1_rt+E2_rt+E3_rt)/3, nurses_3 = (B4_rt+B3_rt+B76_rt)/3,
         personal_issues_3 = (F2_rt+F41_rt+F1_rt)/3, test_3 = (D3_rt+D4_rt+D65_rt)/3) #averages post-trans

train %<>% mutate(personal_issues_3_rt = personal_issues^(1/6),
                  F68_rt = sqrt(F68))

```

## Results

The final regression is modeled by

$$ f(x) = -3.52083 + 1.94361*x_1 + 1.35390*x_2 + 1.11454*x_3 $$ $$ RMSE = .12 $$

where β~1~ describes change in ŷ for a one unit change in the root mean satisfaction for the personal issues category, β~2~ for a one unit change in the root mean satisfaction for the nurses category, and β~3~ for a one unit change in the root indicator for Doctor's concern for patients comfort while treating them. Interpretations for β~0~ is omitted due to transformations and range of the data set. The Adjusted R^2^ \~ .74, F(3,2045), p = 2.2e-16 using adjusted Type I error rate = .005.

Our model produced a MSE = 0.015 (RMSE = 0.123) with statistically significant results for all predictors. **The RMSE quantifies the models predictive capability in the context of regression such that it measures the average difference between the observed values and the predicted values of our model. In other words, the model predicts overall satisfaction within .12 of the actual satisfaction scores for the training data set.** Although robust and promising, true confirmation of performance is assessed on the training data set via precision grades below. 

```{r}

model <- lm(F68_rt~personal_issues_3_rt+nurses_3+C4_rt, data = train)
anova(model)

```

```{r, echo = FALSE, eval = FALSE, include = FALSE}

model2 <-  lm(F68_rt~personal_issues_3+nurses_3+C4_rt_6, data = train)
anova(model2)
```


*If* the data was obtained via random sampling, the model then could be used as an estimate for the true target population's overall satisfaction. Residual plots were used to test residual normality and linearity with P(F) = .57 on one degree of freedom for Tukey's test for non-additivity. Residual values lie within three standard deviations with a few minor exceptions in the fitted values, prompting an opportunity for different optimization methods. The apparent uncaptured variation between the predictor variables calls for models suited for continuous ordinal data such as logistic, poisson, or generalized linear models that can capture different types of distributions.

```{r}

residualPlots(model)

```

```{r}
influenceIndexPlot(model)

mydata %>%
  filter(survey_id %in% c(1344260683,1439794901))

mydata[mydata$survey_id==1344260683,]

4/dim(train)[1]

```

Survey IDs 1344260683 and 1439794901 had the highest influence on the overall model in comparison to the global average of Cook's distance. Looking at the original data, the former had missing responses for four of the seven questions used to produce the model, highlighting a major flaw in this model - its predictive inaccuracy for patients with missing responses - as missing responses were filled with grand means after exclusion criteria were applied. Although missing at random, a better approach would be to fill in with median values or apply more conservative exclusion factors such as omitting observations with greater than 10% of responses missing. The latter was greatly considered, but the trade-off of small sample sizes and subsequent over fitting were outweighed.

After confirming the absence of variance inflation, the Log Likelihood ratio test was done for the nested model excluding x~3~, as this was the only non-summary predictor. Wald statistics are limited for linear models such that P(Wald) \> P(LRT). Assuming null hypothesis is true where the nested model is just as adequate as the full model, p(χ) = 8.3e-45 (204,2) the affect of x~3~ on the model is statistically significant.

```{r}

modelx <- lm(F68~personal_issues_3+nurses_3, data = train)
-2*(logLik(modelx)-logLik(model))
pchisq(203,2,lower.tail = FALSE)

```

### (cont.)
