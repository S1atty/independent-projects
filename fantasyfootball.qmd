---
title: "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams"
author: "Tomi Akisanya"
output: html_document
editor_options: 
  settings:
  chunk_output_type: console
  canonical: TRUE
---
```{r setup,eval = TRUE, echo = FALSE, include=FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, cache=FALSE)
```


```{r, eval = TRUE, echo = FALSE, include = FALSE, warning=FALSE}
library(readr)
library(lessR)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(magrittr)
library(rvest)
library(openxlsx)
library(car)
library(psych)
library(corrplot)
library(DT)
library(reshape2)
library(knitr)
# install.packages(c("corrr","ggcorrplot","FactoMineR"))
library(corrr,ggcorrplot)
# install.packages("factoextra")
library(factoextra) # pca visualization
# library(gt) # 'beautiful tables'

```

Around this time every year, if you're like me, you've already started to mentally prepare for 80% of your conversations to be on the topic of football. For those individuals that would not classify themselves as ffanatics, it's probably annoying - *it's annoying for all of us*. Pretty consistently there will behavior from fans that range from DMs to athletes, verbal assaults to close friends, damaged property, and public humiliation. Who realistically has the mental endurance to only discuss a single topic over an extended period of time for something they're not physically involved in? It's those with passion, and although I am not passionate about football (nor justify the aforementioned behavior), it will be my first of hopefully many Fantasy Football leagues. If there's one thing I am passionate about, it's tilting the odds in my favor. Usually that's in the form of taking a creative line on the felt on a 2/5 reg by extracting max value with 67s on a low connected board in a 4! pot as the preflop aggressor. Balance and discipline is the name of the game though, and who are we if we don't apply the same approach to all areas in life...


# Data Set

The data contains in-game and Fantasy Football Points per Reception stats by NFL player from 2017 - 2023 for all 17 games of the regular season. Most leagues use a points per reception based metric to calculate fantasy points, or `FantasyPointsPPR.` Before converting to fantasy points, in-game stats may be weighted or counted differently. My league adopted the following criterion: 

```{r}
league_weights <- data.frame(read.xlsx("Fantasy Football Data/archive/fantasy_strata_league_scoring.xlsx"))
knitr::kable(league_weights[,-3])
```

Two different data sets are used, with a focus on three distinct NFL regular seasons - 2022, 2023, and 2024. Both data sets have been scraped but differ in source, purpose, and underlying information present:

Data Set 1 - Historical - 2022 & 2023: This data set contains historical data from 2017-2023 for both relevant in-game statistics and fantasy scoring for  regular NFL season. This project primarily focuses on 2022 and 2023. Each observation or row in the data set is a NFL athlete's relevant in-game statistics, such as position, team, completion, attempts, interceptions per attempt, etc. Since most leagues exclude defensive players from their fantasy team, those have been implicitly removed from the data set. The key feature of this data set are the retroactive fantasy rankings/scoring. The total fantasy points, overall rank, and rank by position are available for each player. This enables the direct comparison of calculated rankings from models to their actual rank.

Players that were on two or more teams in a given season are not assigned a team - but are instead given a makeshift name to highlight this. For example, you may see Baker Mayfield's registered team as `2TM.` One notable variable contained in the data set is ADP, or average draft position, representing the number of times the player was drafted across all recorded leagues before the start of the respective season. Additional in-game stats were calculated afterwards. These were `YardsPerRushAttempt`,`CompletionsPerAttempt`, `TDsPerAttempt`, `InterceptionsPerAttempt`, `TDsPerReception`, and `FumblesLostPerFumble.` My fantasy league's scoring methodologies were also factored into a set of new variables. These variables have the `schoring_` schema. 

Data Set 2 - Projected - 2024: This data set contains all player match ups for the upcoming regular 2024 NFL season. The in-game statistics recorded for each player are projections based on those teams and match ups. One benefit of this data set is being able to use these projections as inputs of our model to determine which players obtain the most fantasy points, rank them in ascending order, and draft them accordingly. The drawback is that no additional information on how these projections were calculated are known so the accuracy of these projections cannot be confirmed. 

# Methods

The current methodology is to use singular value decomposition of eigenvalues to create scores of new variables that can be attributed to their overall performance. Their projected overall performance would be used to rank each player in ascending order (potentially by position) to inform our draft decision. Starting with the 2022 season, the overall rank for each player is calculated and then compared to the actual ranks of that same year. Precision will be measured in three ways: 

1. Difference in overall rank
2. Difference in position rank
3. Difference in fantasy points, obtained by taking the difference of fantasy points using our model's draft order with the fantasy points using the ideal draft order.

If the model is precise, the same 2022 projections will then be tested on 2023. This method is not full-proof obviously. Many things change between off seasons of professional sports, but the objective is to quantify the model's ability to generalize onto future seasons. If it can, the same process will be done starting with the 2023 data set, testing it against itself, then using those scores for 2024. If it does not, principal component scores will be calculated using the 2024 data set only, and only those will inform our draft order. This is not the ideal scenario, since it inherently trusts the projected data. 

```{r, eval = TRUE, echo = FALSE, include=FALSE}

fantasy_merged_17_22 <- read_csv("Fantasy Football Data/archive/fantasy_merged_7_17.csv")
fantasy_adp_17_22 <- read_csv("Fantasy Football Data/archive/adp_merged_7_17.csv")
fantasy_adp_17_22 <- data.frame(fantasy_adp_17_22)
mydata_id <- read_csv("Fantasy Football Data/archive/adp_merged_7_17.csv")
# fantasy_test_23 <- read.xlsx("fantasy_mergedsourcecleaned_2023.xlsx") 2023 test

mydata <- fantasy_merged_17_22 %>%
  left_join(fantasy_adp_17_22 %>%
              select(PlayerID,adp,Year), by = c("PlayerID","Year")) %>%
  data.frame()

fantasy <- mydata %>%
  rename(
    Rank = Rk,
    Player = Player,
    Team = Tm,
    FantasyPosition = FantPos,
    Age = Age,
    Games = G,
    GamesStarted = GS,
    Completions = Cmp, #QB
    Attempts = Att, #QB
    PassingYards = Yds, # SCORING: (*1/25)
    PassingTDs = TD, # SCORING: (*4)
    Interceptions = Int, # SCORING: (*-2)
    RushingAttempts = RushAtt,
    RushingYards = RushYds,  # SCORING: (*1/10)
    YardsPerAttempt = YA, # ***RushingYards / RushingAttempts 
    RushingTDs = RushTD, # SCORING(*6)
    Targets = Tgt, # TE/WR no. times player is thrown ball 
    Receptions = Rec, # no. passes caught; SCORING: (*1)
    ReceivingYards = RecYds, # SCORING: (*.1)
    YardsPerReception = YR, # ***ReceivingYards / Receptions ; compelte rate = .85
    ReceivingTDs = RecTD, # SCORING: (*6)
    Fumbles = Fmb, # SCORING: (*-2)
    FumblesLost = FL, # no times player fumbles and opp team receives; SCORING: (*-2)
    FantasyPointsPPR = PPR, # no fantasy pts in a points per reception league
    PlayerID = PlayerID,
    PositionRank = PosRk, # ranking grouped by position
    Year = Year, 
    ADP = adp
  )

fantasy %<>% mutate(
  YardsPerRushAttempt = RushingYards / RushingAttempts, # duplicate 
  CompletionsPerAttempt = Completions / Attempts,
  TDsPerAttempt = PassingTDs / Attempts,
  InterceptionsPerAttempt = Interceptions / Attempts,
  TDsPerReception = ReceivingTDs / Receptions, # complete r = .85
  FumblesLostPerFumble = FumblesLost / Fumbles, .after = ADP # complete rate = .4
)

fantasy %<>% mutate(Team = factor(Team), FantasyPosition = factor(FantasyPosition))

# including scoring 

fantasy %<>% mutate(
  scoring_PassingYards = PassingYards*.04,
  scoring_PassingTDs = PassingTDs*4,
  scoring_Interceptions = Interceptions*(-2),
  scoring_RushingYards = RushingYards*(.01),
  scoring_RushingTDs = RushingTDs*6,
  scoring_Receptions = Receptions*1,
  scoring_ReceivingYards = ReceivingYards*.1,
  scoring_ReceivingTDs = ReceivingTDs*6,
  scoring_Fumbles = Fumbles*(-2),
  scoring_FumblesLost = FumblesLost*(-2)
)


```

# Processing 

Missing values in completions per attempt, tds per attempt, interceptions per attempt, tds per reception, and fumbles lost per fumble were a result of undefined values in the denominator. Observations of this missing values are directly related to the player and position. For example, QBs will rarely record touchdowns per reception and will therefore have undefined values for those statistics since they are more equipped to measure performance of wide receivers. All missing values in these cases were replaced with zero. The same approach was applied to yards per attempt and yards per reception with two notable exceptions. Foster Moreau had zero rushing attempts but two rushing yards during the 2022 NFL season which is difficult to interpret considering rushing attempts are a function of rushing yards. In the same vein, Joe Flacco had -3 receiving yards but zero receptions. Both of these players were removed from the data set. 

```{r, eval=FALSE}


fantasy %>%
  select(-ADP,-YardsPerAttempt) %>%
  filter(if_any(everything(),is.na)) %>%
  tibble() 

# YardsPerAttempt - RushingYards / RushingAttempt

fantasy %>%
  filter(!is.na(YardsPerAttempt) & RushingYards!=0) %>%
  slice_sample(n=5) %>%
  summarize(n=RushingYards,n2=RushingAttempts,n3=YardsPerAttempt,n4=RushingYards/RushingAttempts) # data quality check
fantasy %>%
  filter(is.na(YardsPerAttempt),
         (RushingYards!=0 & RushingAttempts==0)|(RushingYards==0 & RushingAttempts!=0)) %>%
  select(Player,PlayerID)
# Foster Moreau MoreFo00 has 0 rushing attempts but 2 rushing yards. Will most likely remove player from data set. 
# all other players are na(YardsPerAttempt) since RushingYards = 0 and RushingAttempts = 0

# YardsPerReception - ReceivingYards / Receptions
fantasy %>% filter(is.na(YardsPerReception)) 
fantasy %>%
  filter(!is.na(YardsPerReception) & Receptions!=0) %>%
  slice_sample(n=5) %>%
  summarize(n=Receptions,n2=ReceivingYards,n3=YardsPerReception,n4=ReceivingYards/Receptions)
fantasy %>%
  filter(is.na(YardsPerReception),
         (ReceivingYards!=0 & Receptions==0)|(ReceivingYards==0 & Receptions!=0)) %>%
  select(Player,PlayerID)
# Joe Flacco FlacJo00 - -3 receiving yards but 0 receptions; doesn't make sense depends on how its counted

# CompletionsPerAttempt
fantasy %>% filter(is.na(CompletionsPerAttempt))
fantasy %>%
  filter(!is.na(CompletionsPerAttempt) & Attempts!=0) %>%
  slice_sample(n=5) %>%
  summarize(n=Completions,n2=Attempts,n3=CompletionsPerAttempt,n4=Completions/Attempts)
fantasy %>%
  filter(is.na(CompletionsPerAttempt),
         (Completions!=0 & Attempts==0)|(Completions==0 & Attempts!=0)) %>%
  select(Player,PlayerID) # no results; substitute zero for NaN

# TDsPerAttempt
fantasy %>%
  filter(is.na(TDsPerAttempt),
         (PassingTDs!=0 & Attempts==0)|(PassingTDs==0 & Attempts!=0)) 

# InterceptionsPerAttempt
fantasy %>%
  filter(is.na(InterceptionsPerAttempt),
         (Interceptions!=0 & Attempts==0)|(Interceptions==0 & Attempts!=0)) 

# TDsPerReception
fantasy %>%
  filter(is.na(TDsPerReception),
         (ReceivingTDs!=0 & Attempts==0)|(ReceivingTDs==0 & Attempts!=0))

# FumblesLostPerFumble
fantasy %>%
  filter(is.na(FumblesLostPerFumble),
         (FumblesLost!=0 & Fumbles==0)|(FumblesLost==0 & Fumbles!=0)) 

```


```{r}

# remove joe flacco and foster moreau
fantasy <- fantasy[!fantasy$PlayerID %in% c("FlacJo00","MoreFo00"),]

# remove duplicate variable YardsPerRushAttempt
fantasy %<>% select(-YardsPerRushAttempt)
# replace NaN with 0 
sum(is.na(fantasy$YardsPerAttempt))

for (i in seq_along(1:nrow(fantasy))){
  for (j in seq_along(1:ncol(fantasy))){
    if (any(is.na(fantasy[i,j])) & j != 28){
      fantasy[i,j] <- 0
    }
  }
}

```

# YoY PPR Trend for Top NFL Teams

```{r, eval = TRUE, echo = FALSE, fig.width=15,fig.height=10}


fantasy %>%
  select(Year,Team,FantasyPointsPPR) %>%
  group_by(Year,Team) %>%
  summarize(n=sum(FantasyPointsPPR)) %>%
  slice_max(n, n = 5) %>%  # PHI, KAN, MIN, JAX, CIN
  ggplot(aes(x = n, y = factor(Year), fill=Team, label = Team, color = Team)) +
  geom_col(position = "dodge", show.legend = FALSE) +  
  coord_cartesian(xlim=c(1300,1900))+
  scale_fill_manual(values=c("KAN"="red4"))+
  scale_color_manual(values=c("KAN"="red4"))+
  geom_text(position = position_dodge(width = 0.9), hjust = -0.1, show.legend = FALSE) + 
  labs(x = "Fantasy Points PPR", y = "Year", title = "Fantasy Points by Team and Year") +
  theme_minimal() +  # Optional: Apply a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(color = "red4")) 

```

# 2022 NFL season 

## Data Exploration

The data was sub set for the 2022 season and then skimmed for to review distributions, counts, and other elements within the data set. 

```{r}

fantasy <- fantasy %>%
  filter(Year == 2022)
skimr::skim(fantasy)
```

Important categorical variables to note outside of the Team and Player is the Fantasy Position. These are QBs, WRs, TEs, and RBs. All defensive positions are not scoped and have been explicitly removed from the data set. The objective of this project is to create a fantasy team that has the highest likelihood of obtaining the most Fantasy Points (PPR) for the upcoming season based on prior seasons. The next logical question becomes, does this likelihood vary by team? By position? Intuitively, the likelihood will vary simply based on how many of these positions are in game at a given time. A table of each position shows the distribution of each position in a given NFL season.  

```{r}
fantasy %>%
  janitor::tabyl(FantasyPosition) %>%
  arrange(desc(percent))
```

A majority of NFL players in 2022 for fantasy purposes were wide receivers (38%) while the least common position were quarterbacks (14%). For the continuous variables, the important things to note are the means, standard deviations, their counts and distribution via the histograms, and any missing values. In-game stats such as Yards Per Attmept, Yards per Reception, and additional variables (below ADP) are missing for a lot of players as expected. The missing values are likely due to a number of factors such as the position of the player and the number of games each played. The range of games played by each player vary from none to 17, representative of 17 total games in the regular season. The descriptive summary for Fantasy Points (PPR) is: 

```{r}
psych::describe(fantasy$FantasyPointsPPR)
```

With mean = 78.4 (sd = 85.29), it is obvious that there is high variability. The histogram of PPR shows where how this variability is distributed. 

```{r}
hist(fantasy$FantasyPointsPPR, main = "Histogram of Fantasy Points (PPR)",
     xlab = "PPR", ylab = "Count")
```

Depending on the fantasy league, PPR scoring will be different in the sense that there is typically a PPR threshold per game for a player in order for their PPR to be recorded for that given week. If the player is below the baseline, the PPR may be zero. Interestingly enough, the distribution of PPR in 2022 is a log-normal right skew distribution. The observed PPR for most players were approximately zero. This is logical, granted the extreme difficulty of being a top NFL performer. Most players are benched, aren't playing games, nor are starters. Evidently, most of the continuous variables will follow the same trend with observed frequencies ~0 and relatively a fewer number of players scoring the most for each variable. There are three notable exceptions: 

```{r,fig.align='center'}

h_adp1 <- 
fantasy %>%
  ggplot(aes(x=ADP))+
  geom_histogram(bins = 8, color = "black")+
  scale_x_continuous(name = "ADP")+
  scale_y_continuous(name = "Count")+
  labs(title = "Average Draft Position")
  
h_adp2 <- 
fantasy %>%
  ggplot(aes(x=FumblesLostPerFumble))+
  geom_histogram(bins = 9, color = "black")+
  scale_x_continuous()+
  labs(title = "Fumbles Lost per Fumble")+
  theme(axis.title.y = element_blank())
h_adp3 <-  
  fantasy %>%
  ggplot(aes(x=YardsPerReception))+
  geom_histogram(bins = 11, color = "black")+
  labs(title =  "Yards Per Reception")+
  theme(axis.title.y = element_blank())

gridExtra::grid.arrange(h_adp1,h_adp2,h_adp3, ncol = 3)

```

1. Average Draft Position - Uniform Distribution: For players that have this data available, ADP is uniformly distributed across most players. This implies that the parent population drafts players evenly across the board and there isn't a strong concentration of players being picked predominantly. To reiterate, this is specifically for players that have ADP data available, which may be a combination of the most popular or the best players. 

2. Fumbles Lost per Fumble - Bimodal distribution: Frequency of Fumbles Lost per Fumble have peaks at both zero and one, revealing that most players either fumble and lose possession of the ball, resulting in a turnover, or don't fumble at all. 

3. Yards Per Reception - Normal distribution: Players average 10 Yards Per Reception with a majority of players within the range of two standard deviations from the mean. Although there is a slight right skew, this is one of the few variables that follows a Gaussian distribution. 

The first thing we want to understand is *what* position, if any, we should be more inclined to draft first, and their likelihood of obtaining the most fantasy points. I started by seeing what the public likes to draft first. In 2022, the public drafted quarterbacks around 86 times on average, the highest of the four positions, with tight ends at a close second of 84 times on average. Running backs and wide receivers then followed. Acknowledging that ADP data is not available for all positions, I wanted to better understand if quarterbacks are the biggest factor in regards to PPR. A comparison of ADP and PPR by position begins to paint the picture. On average, quarterbacks had 105 fantasy points in 22, followed by wide receivers, running backs, then tight ends.

```{r}

fantasy %>%
  select(FantasyPosition,FantasyPointsPPR,ADP) %>%
  group_by(FantasyPosition) %>%
  summarize(AverageDraftPosition=mean(ADP, na.rm=TRUE),
            AveragePPR=mean(FantasyPointsPPR, na.rm=TRUE)) %>%
  arrange(desc(AveragePPR)) 
# QB / WR / RB / TE -- avg PPR
# wr / rb / qb / te -- total (totals are void see below)

```

This seems to be consistent at a higher level when the top three teams for the 2022 regular season are compared. 

```{r, eval = TRUE, echo = FALSE, fig.width=15,fig.height=10}
fantasy %>%
  filter(Team %in% c("PHI","KAN","CIN")) %>%
  select(Team,FantasyPosition,FantasyPointsPPR) %>%
  group_by(Team,FantasyPosition) %>%
  summarize(n=mean(FantasyPointsPPR),
            n2=sum(FantasyPointsPPR)) %>%
  slice_max(n, n = 5) %>%
  arrange(desc(n)) %>%
  ggplot(aes(y=fct_rev(fct_inorder(FantasyPosition)),x=n,fill=Team,color=Team, label = Team)) +
  geom_col(position = "dodge") +  
  scale_fill_manual(values=c("KAN"="red4","PHI"="palegreen4","CIN"="slategray4"))+
  scale_color_manual(values=c("KAN"="red4","PHI"="palegreen4","CIN"="slategray4"))+
  # coord_cartesian(xlim=c(1250,2000))+
  geom_text(position = position_dodge(width = 0.9), hjust = -0.1) +  # Add text labels
  labs(x = "Fantasy Points PPR", y = "Year", title = "Fantasy Points by Team and Year") +
  theme_minimal() +  # Optional: Apply a minimal theme
  theme(axis.text.x = element_text(angle = 45, hjust = 1,size=12),
        axis.text.y = element_text(size=12),
        legend.position = "top") 
```


However, properly accounting for total number of games played by player, yields different results. Evidently, quarterbacks are disproportionately the most efficient in regard to fantasy points on a per game basis at .164 per game. Running backs, tight ends, and receivers consecutively follow but are significantly behind. However, fantasy points between those three positions vary by less than 5%.  The logical question becomes what are the underlying causes of this variation? One possibility is that there are typically more wide receivers on an offensive play than any other position. Another possibility can be due to the extreme routes and distances wide receivers run, making them injury prone and therefore less efficient. 

```{r}

fantasy %>%
  group_by(FantasyPosition) %>%
  summarize(PPRperGame = mean(FantasyPointsPPR,na.rm = TRUE)/sum(Games,na.rm=TRUE)) %>%
  arrange(desc(PPRperGame)) # PPR efficiency (after accounting for # of games played)
# QBs have most efficient PPR on per game basis, WRs seem to have the lowest impact on per game basis
# After accounting for no. of games, QB > RB > TE > WR  

```

## Analysis SVD PCA 

Eigenvalue decomposition is an unsupervised machine learning method typically used for dimensionality reduction on mostly unlabeled data. The same approach is used here, with the intent to reduce the data to components that measure performance in some aspect. The best way to think about this is in the form of a recipe. A cookbook's recipe for chicken cordon bleu will have elaborate concoctions and mixes of different food. When applied in this context, it would reduce the recipe to its core components, 3/4 chicken, 1/8 cheese, 1/8 ham let's say. It then becomes much easier to make chicken cordon bleu while keeping most of the taste. While an oversimplification, the approach is essentially the same, with the goal of maximizing the amount of underlying variation using linear combinations of variables. At its core, there is some latent underlying variable(s) that combinations of these variables measure. What those underlying variables measure and its relevancy is on us to define. These are the principal components. The principal components are made up of the original variables, and how much that variable contributes to the underlying variable (ie principal component) are the eigen vectors or *loadings*. Loadings can be positive (greatly contributes) or negative (adversely contributes). How well the variables load help define what that new underlying variable is. To define the inclusion criteria, any variable that loads +/- .7 will be considered as loading well and those variables alone will be what is used to define the underlying variable/component. This empirical threshold is a very conservative approach. 

```{r}

## 1) Scale, remove ADP, Year, Rank, PositionRank, (maybe Games and Age); 2) create correlation matrices

# og x_vars
ffnorm_og <- fantasy %>%
  select(where(is.numeric) & -ADP & -Year & -FantasyPointsPPR & -Rank & -PositionRank &
         !contains("scor", ignore.case = TRUE)) %>%
  mutate_all(scale.default)
# -- og cor matrix
fantasy_cormatrix_og <- 
  fantasy %>%
  select(where(is.numeric) & -ADP & -Year & -FantasyPointsPPR & -Rank & -PositionRank &
         !contains("scor", ignore.case = TRUE)) %>%
  cor()

# all x_vars
ffnorm_all <- fantasy %>%
  select(where(is.numeric),-ADP,-Year,-FantasyPointsPPR,-Rank, -PositionRank) %>%
  mutate_all(scale.default)
# -- all cor matrix
fantasy_cormatrix_all <- fantasy %>%
  select(where(is.numeric),-ADP,-Year,-FantasyPointsPPR,-Rank, -PositionRank) %>%
  cor()

# scoring vars
ffnorm_score <- fantasy %>%
  select(contains("scor", ignore.case = TRUE)) %>%
  mutate_all(scale.default)
# -- scoring cor matrix
fantasy_cormatrix_scoring <- fantasy %>%
  select(contains("scor", ignore.case = TRUE)) %>%
  cor()

```

Eigen vectors describe a mathematical phenomena such that 

$$ A * v =  λ * v $$

where A is a square matrix, v is an eigen vector, and λ is a scalar (numerical value) and the associated eigen value of vector v. In this application, matrix A is correlation matrix of the original data. This mechanism works because linear transformations are applied to the data meaning the data does not inherently change. The proportions of all variables and the direction in which they move remain the same. The data gets centered at the origin after scaling, and a best fitting line is calculated that goes through the origin and maximizes the variance in the data. The algorithm does this by fitting a random line through the data, projecting the points onto the line, and calculating the largest sum of squared difference. The yielded line of best fit is the eigen vector for the principal component and the slope is the eigen value. 

```{r,fig.align='center',fig.width=8,fig.height=6}

#scree_plot1 <- fa.parallel(fantasy_cormatrix_og,n.iter=100,fa="pc", main="Parallel Scree")
# parallel test suggests pc1-pc3 on conservative side; scree test suggest up to pc5
# psych::scree(fantasy_cormatrix_og,pc=TRUE,factors=TRUE) 



pc <- prcomp(ffnorm_og,scale. = FALSE)
eigenvalues <- pc$sdev^2
eigen_df <- data.frame(PC = paste0("PC", 1:length(eigenvalues)), Eigenvalue = eigenvalues)
eigen_df$PC <- factor(eigen_df$PC, levels = paste0("PC", 1:length(eigenvalues)))
scree_plot1 <- 
eigen_df %>%
  summarize(PC=PC,Eigenvalue=Eigenvalue) %>%
  arrange(desc(Eigenvalue)) %>%
  ggplot(aes(x = PC, y = Eigenvalue)) +
 # geom_bar(stat = "identity", fill = "steelblue") +
  geom_line(group = 1, color = "red") +
  geom_point(color = "red") +
  geom_segment(x=0,y=2,xend=24,yend=1,linetype = "dotted",linewidth = 1)+
  annotate("label",x=4,y=2.5,label="Parallel Test",color="black")+
  labs(title = "Scree Plot", x = "Principal Components", y = "Eigenvalues") +
  theme_minimal()
# scree as a % of total variation
scree_plot2 <- 
prcomp(fantasy %>%
  select(where(is.numeric) & -ADP & -Year & -FantasyPointsPPR & -Rank & -PositionRank &
         !contains("scor", ignore.case = TRUE)), scale. = TRUE) %>%
  fviz_eig(addlabels = TRUE)

gridExtra::grid.arrange(scree_plot1,scree_plot2,ncol=2)

pc_og <- principal(ffnorm_og,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") # default is varimax rotation
# pc_og$loadings
# print(as.matrix(pc_og$loadings), cutoff = 0) # seems to only account for 62% of the total variance 
# eigen(fantasy_cormatrix_og)$val/24
# 0.55743682334+0.37837352942+0.21494701803
# fantasy %>%
#   group_by(FantasyPosition) %>%
#   summarize(n=mean(FumblesLost))
# # PC1 = The QB Effect 
  
# fviz_pca_var(prcomp(ffnorm_og, scale. = FALSE), col.var = "black")

```

A parallel test was used to measure the number of components to obtain. The test performs the same decomposition on simulated data of the same size and graphs the results. Where the simulated and actual data intersect is the cutoff for the number of components to obtain. The results of the test suggest three components. The y-axis plots the eigenvalues which is the total variation explained by each component. In a simpler sense, it can be thought of as the number of original variables accounted for in that component (hence the horizontal line separating values less than one). Three principal components were obtained, acknowledging that principal component one (PC1) should account for approximately six variables, PC2 around 5, and PC3 around 3. The other scree plot better highlights the components as a percentage of total variability explained. Keep in mind that PC1 only accounts for 30% of the total variability and the first three components cumulatively account for 52% of total variability. It's likely that the post-hoc tests described in the methods section will not be sufficient for our goal since there is still half of the total variation not accounted for in these components. 

Scores are calculated for each individual player. Depending on how the components are defined, players can be ranked in ascending order. 

```{r,fig.width=12,fig.height=10,fig.align='center'}
test <- data.frame(pc_og$loadings[,c(1,2)])



test$variable <- row.names(test)
test_melt <- melt(test,id.vars = "variable")
colnames(test_melt) <- c("variable","component","value")
test_melt %>%
  mutate(component=fct_recode(component,PC1="RC1",PC2="RC2")) %>%
  ggplot(aes(x=variable,y=component,fill=value,label=round(value,2)))+
  geom_tile(color = "white") +
  geom_text(size=3,face="bold",color="black")+
  scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name="Loading") +
  theme_bw()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1),
        axis.title.x = element_blank()) +
  labs(title = "Principal Component Loadings",y="Principal Components")

```


## Principal Components - In-Game Stats
rotation: maximum variance 

**PC1 Definition: High-Volume Performing QBs**\
Attributes: Completions, Attempts, Passing Yards, Passing TDs, Interceptions, Fumbles\

1. PC1 would be attributed to high-volume QBs as they load extremely well for the above categories. The first inclination was to attribute PC1 to high performing quarter backs, however, that statement alone would be unjustifiable considering that interceptions and fumbles load extremely well to this component. High-volume quarter backs would be a more fitting description. These quarter backs are performing extremely well in some regard since it loads high for completions, passing yards, and passing touch downs. We can reason that these quarter backs are also able to consistently get the ball off of their hands. High-volume quarter backs will also load high to interceptions and fumbles. The more throws and attempts made, the more likely that fumbles and interceptions will occur. 
  
**PC2 Definition: Offensive Long Range Efficiency**\
Attributes: Games Started, Targets, Receptions, Receiving Yards, Receiving TDs\

2. PC2 can be attributed to total overall offensive efficiency given that we load extremely high for targets, receptions, receiving yards, and receiving    touchdowns. Players typically defined for this category would be wide receivers, and pc2 is measuring yardage efficiency. Efficiency is important in this context given that we also load high to targets, and even though it is not an inclusion criteria in fantasy scoring, it speaks to the aggressiveness on the offensive side. The key distinction to make here is that this describes the overall long range efficiency only, since rushing yards and touch downs are not accounted for in this component. Additionally, *overall* long range efficiency is justified since this must be a combination of quarter backs and the offensive line. Wide receivers, running backs, and tight ends will generally only score more touch downs and have more yards with a good quarter back. 
  
**PC3 Definition: Offensive Driving Efficiency**\
Attributes: Rushing Attempts, Rushing Yards, Rushing TDs\

3. PC3 would be attributed to mainly RBs and TEs that are elite drivers since they load high for rushing stats. 

Ideally, we'd want to load players that load high for all three categories. Considering the nature of football, depending on player's primary position, they will naturally perform better in certain stats or categories over others. In this case, there are multiple approaches to account for this. Actually, along every step I find there are ways that our paths diverge, but more on that later. One approach is to use only principal component one and players/scores that load high for that component to pick our quarter back. Principal components two and three would then be used for all other positions. I started with that approach but here comes the other divergence - how I choose the calculate the scores. There are two options under consideration. 

1. Include all variables in the principal component computation, with the benefit of providing a more comprehensive score but the drawback of added complexity.
2. Include only variables in the principal component computations that load high as the score, with the benefit of exclusively calculating how good they are at being good but the drawback of missing nuanced information capture in less significant variables. 

I tested the model starting with the second approach. First, I reviewed how the top ten quarter backs performed in 2022 by looking at their overall rank, position rank, the player, and the total fantasy points they had. 

```{r}
# test for pc1 
rank_actual <- 
fantasy %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR) %>%
  tibble()

rank_actual[1:10,]
```

Then we use the eigen values to calculate the principal component scores for each player, only including variables that loaded high. The players with the highest scores would be the highest performing QBs predicted for the 2022 season. Two things to note. Firstly, the position and overall rank will be the same here since we've define our first principal component as attributes of quarter backs only. Secondly, the model's fantasy points ppr would not be known. The purpose here is to calculate scores and draft in ascending order. We can, however, calculate the difference in fantasy points had we taken the models' picks. In the table, the model's fantasy points are the same as actual fantasy points to make this calculation easier. The results are shown below.

```{r, eval=TRUE,echo=FALSE,include=FALSE}

rank_pred <- 
fantasy %>%
  mutate(pc1 = (0.96755758*Completions)+(0.96979088*Attempts)+(0.96705865*PassingYards)
         +(0.93954025*PassingTDs)+(0.93859580*Interceptions)+(0.77179023*Fumbles)) %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR,pc1) %>%
  arrange(desc(pc1)) %>%
  tibble() %>%
  rename("Model_OverallRank" = "Rank",
         "Actual_PositionRank"="PositionRank",
         "Model_Player"="Player",
         "Model_FantasyPointsPPR"="FantasyPointsPPR",
         "Model_PCscore"="pc1") %>%
  mutate(Model_PositionRank = c(1:82),.before = Model_Player) %>%
  print(n=10)

rank_diff <- cbind.data.frame(rank_actual[1:10,],rank_pred[1:10,-1])
knitr::kable(rank_diff)

```


Methods of measuring model performance: 

1. **Percentage of total players the model accurately selects.** If we were selecting quarter backs, we would use the first principal component scores to obtain the top 10 quarter backs in ascending order. We would then compare the results against the actual top 10 quarter backs for that season. In these results, 70% (7 out of 10 QBs) were accurately selected as being in the top 10 for total fantasy points. 

2. **Difference in total fantasy points of the top 20 players.** The top 20 players for each position are isolated using their associated principal component scores. The sum of the total fantasy points for the top 20 players are then subtracted from the what the actual total fantasy points for players in the top 20 in each position had to obtain the delta. 

3. **Absolute difference in position rank by player.** Each player will have the net difference in their position rank between the model and their actual rank for that season. In the above table, for example, the model selects Justin Herbert as #2 QB for fantasy but was actually #11 after the regular season, making the net -11.  

Now using the model's picks for the top ten quarter backs, 70% of those selected in the top 10 were actually in the top ten during the 2022 regular season. The total fantasy points for the quarter backs picked by the model were 3131.2. The total fantasy points the top ten quarter backs actually had was 3302 which means the model was off by 5% in regard to quarter back selection. The difference for rank by position are shown below. Players with a negative delta are those that were ranked higher in the model but came in lower after the season.

```{r}

rank_diff %>%
  mutate(PositionRankDelta=PositionRank-Actual_PositionRank) %>%
  tibble() %>%
  select(Model_Player,PositionRankDelta) %>%
  print(n=10)

```

## Principal Components - Fantasy Weights

While the results are seemingly great, this was when I recalled that these principal components only account for half of the total variation. Considering that the goal of singular value decomposition via principal component is to maximize the total variation in the data, I had to think we can do much better. The first principal component which was attributed to quarter backs load high for interceptions and fumbles. While it is justifiable to reason that high-volume quarter backs will naturally intercept and fumble more often solely as a function of volume, would the *best* quarter backs really load high for those? To better understand this, the exact same process above was done to calculate new component scores, this time only including variables that are used for fantasy scoring. I first created these additional variables by multiplying them by my league's point system. For example, rushing touch downs were multiplied by six and became the new variable used for the svd. Those variables were passing yards, passing touchdowns, interceptions, rushing yards, rushing touchdowns, receptions, receiving yards, receiving touch downs, fumbles, and fumbles lost. 


```{r, eval=FALSE,echo=FALSE,include=FALSE}

# pc2
# actual
fantasy %>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR) %>%
  tibble() %>%
  print(n=10) # mostly WR RB like the pc analysis shows; top 10 are evenly split between teh best WR and best TE
# predicted
fantasy %>%
  mutate(pc2 = (0.732909476*Games)+(0.954222398*Targets)+(0.944470501*Receptions)+
           (0.956157602*ReceivingYards)+(0.882981792*ReceivingTDs))%>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc2) %>%
  arrange(desc(pc2)) %>%
  tibble() %>%
  print(n=10)


# pc3
fantasy %>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR) %>%
  tibble() %>%
  print(n=10) 

fantasy %>%
  mutate(pc3 = (0.961141891*RushingAttempts)+(0.966004285*RushingYards)+(0.910680715*RushingTDs)) %>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc3) %>%
  arrange(desc(pc3)) %>%
  tibble() %>%
  print(n=30)

pc_og$loadings[,3]

# using all variables 
fantasy %>%
  mutate(pc1 = pc_og$scores[,1],
         pc2 = pc_og$scores[,2],
         pc3 = pc_og$scores[,3],
         ovr = pc1+pc2+pc3) %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1,pc2,pc3,ovr) %>%
  arrange(desc(ovr)) %>%
  filter(FantasyPosition=="QB") %>%
  tibble() %>%
  print(n=100) %>%
  slice_max(FantasyPointsPPR,n=30) %>%
  summarize(sum(FantasyPointsPPR))
  

fantasy %>%
  mutate(pc1 = (0.96755758*Completions)+(0.96979088*Attempts)+(0.96705865*PassingYards)
         +(0.93954025*PassingTDs)+(0.93859580*Interceptions)+(0.77179023*Fumbles)) %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR,pc1) %>%
  arrange(desc(pc1)) %>%
  # tibble() %>%
  # print(n=30) %>%
  slice_max(pc1,n=30) %>%
  summarize(sum(FantasyPointsPPR))

```


### Results

The parallel test suggested three components were sufficient to explain the maximum variation in the data. 

```{r,fig.align='center'}

prcomp(ffnorm_score,scale.=FALSE) %>%
  fviz_eig(addlabels=TRUE)

```

The first three components alone account for 88.1% of the total variation within the data set, much better than the 50% obtained previously. The mean item complexity = 1.1. This means that each individual variable included in the principal components only load significantly on one component. This is the more ideal scenario since it makes defining the components much easier. Previously we had a mean item complexity of 1.5, meaning that half of the variables on average load significantly to two components. The first three components are then defined using a loading threshold of .7. 

```{r,fig.width=12,fig.height=10,fig.align='center'}

pc_scoring <- principal(ffnorm_score,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") 

test <- data.frame(pc_scoring$loadings[,c(1:3)])
test$variable <- row.names(test)
test_melt <- melt(test,id.vars = "variable")
colnames(test_melt) <- c("variable","component","value")
test_melt %>%
  mutate(component=fct_recode(component,PC1="RC1",PC2="RC2",PC3="RC3")) %>%
  ggplot(aes(x=variable,y=component,fill=value,label=round(value,2)))+
  geom_tile(color = "white") +
  geom_text(size=3,face="bold",color="black")+
  scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), space = "Lab", 
                       name="Loading") +
  theme_bw()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1),
        axis.title.x = element_blank()) +
  labs(title = "Principal Component Loadings",y="Principal Components")

```

### Dimension Definitions
rotation: maximum variance 

**PC1 Definition: Low Performing QBs**\
Attributes: (-) Passing Yards, (-) Passing Touchdowns, (+) Fumbles, (+) Interceptions\

1. PC1 would be attributed to quite literally the least performing quarter backs. The significantly negative loadings for passing yards and touch downs mean that quarter backs that load high to this component are unable to score touch downs. Additionally, they load extremely high for fumbles and interceptions, a confirmation of their under performance in relevant categories. PC1 would only be attributed to quarter backs since these in-game stats are generally relevant to them alone. 
  
**PC2 Definition: High-Performing Distance Efficiency**\
Attributes: + Receiving Touchdowns, + Receiving Yards, + Receptions\

2. PC2 can be attributed to total overall offensive efficiency given that we load extremely high for receiving touchdowns, receiving yards, and receptions. Players that load high to this category are likely wide receivers since wide receivers are more used for long range plays. The key distinction to make here is that this describes the overall long range efficiency only, since rushing yards and rushing touch downs are not accounted for in this component. Additionally, *overall* long range efficiency would better describe this component, since this must be a combination of quarter backs and the offensive line. Wide receivers, running backs, and tight ends will generally only score more touch downs and have more yards with a better quarter back. 
  
**PC3 Definition: High-Performing Driving Efficiency**\
Attributes: + Rushing Touchdowns, + Rushing Yards\

3. PC3 would be attributed to overall driving efficiency in the same fashion. Likewise, this is also a combination of the offense line and the quarterback, considering that high performing drivers will still be unable to score touch downs in some fashion if their quarter back cannot perform. I would expect running backs and tight ends to load high to this category.  

The goal is to isolate players in these areas. 


```{r,fig.width=12,fig.height=10,fig.align='center'}
s <- prcomp(ffnorm_score,scale.=FALSE)

fviz_pca_biplot(s,
                    # Individuals
                
                    geom.ind = "point",
                    geom = c("arrow", "point"),
                    arrow.color = "black",
                    label = "var",
                    fill.ind = fantasy$FantasyPosition,
                    col.ind = "black",
                    pointshape = 21, 
                    pointsize = 1,
                    palette = "jco",
                    addEllipses = TRUE, 
                    repel = TRUE,
                    # Variables
                    alpha.var = "contrib", 
                    col.var = "black",
                    gradient.cols = "RdYlBu") +
  geom_rect(aes(xmin = -13.5, ymin = 0, xmax = -7.5, ymax = -3.5), 
            color = "grey30", 
            alpha = 0, 
            show.legend = FALSE) +  # Hide legend for this rect
  annotate("text", label = "Best Overall QBs", x = -12, y = -0.5, color = "grey30") +
  geom_rect(aes(xmin = 0.25, ymin = 1, xmax = 1, ymax = 1.75), 
            color = "grey30", 
            alpha = 0, 
            show.legend = FALSE) +  # Hide legend for this rect
  annotate("text", label = "Best Offensive \n long range players", x = -2.5, y = 2.5, color = "grey30") +
  geom_curve(aes(xend = 0.8, yend = 1.85, x = -2.8, y = 2.8), 
             color = "grey30", 
             arrow = arrow(), 
             curvature = -0.4, 
             show.legend = FALSE) 

```

The coordinate plane shows the first component on the x axis and the second on the y axis. The `scoring_` and associated arrows are the eigen vectors on this principal component space. An increase on the x axis, or the first principal component, we increase in *under* performance. We would want to obtain players that negative contribute to this component ie quadrant three. In the same fashion, an increase on the y axis means an increase in long range efficiency. This can only show the first two components. Based on the above, the we'd use the first component for quarter backs, the second for wide receivers, and the third for tight ends and running backs. However, since WRs, TEs, and RBs are much more similar in position (which the model concurs via the boxed region in quadrant I) than QBs, those three positions were included and ranked for PC2 and PC3. The allows us to more effectively see the primary position and players the model decides to pick for each category. In summary, principal component one was used for quarter backs, principal components two and three were used for all other positions at first. An overall score was then calculated using PC2 and PC3 only. Players with the highest overall scores would load significantly well to PC2 and PC3. These players would be both the best of the best in both long-range and driving efficiency. This allows us to see what positions the model picks for long-range (PC2) and driving (PC3) efficiency. Note that this tells us what position would be the best at both but *does not* tell us if they are the best at both. 

```{r}
fantasy %>%
  mutate(pc1 = pc_og$scores[,1],
         pc2 = pc_og$scores[,2],
         pc3 = pc_og$scores[,3],
         pc2_3 = pc2+pc3,
         ovr = pc1+pc2+pc3) %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1,pc2,pc3,pc2_3,ovr) %>%
  filter(FantasyPosition!="QB") %>%
  group_by(FantasyPosition) %>%
  summarize(n=mean(pc2_3)) %>%
  arrange(desc(n))
```

We see that running backs and wide receivers on average have positive scores for both, meaning that they contribute positively to the second and third components (long and driving efficiency). Surprisingly enough, tight tends on average are negative in both regards. The model rarely picks tight ends at all across all three of these dimensions. The model tries to pick the best overall QBs via the lowest component scores for PC1, the best wide receivers for PC2, and the best RBs for PC3. That processed was followed with plans of addressing the 'tight end' problem afterwards. The model's results for QBs, WRs, and RBs are below.

### Model Picks
#### PC1 - QBs

```{r}
pc_scoring <- principal(ffnorm_score,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") 

# fantasy %>%
#   mutate(pc1 = (-.956*scoring_PassingYards)+(-.94*scoring_PassingTDs)+(.933*scoring_Interceptions)+
#            (0.850*scoring_Fumbles)+(0.711*scoring_FumblesLost)) %>%
#   filter(FantasyPosition=="QB") %>%
#   select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1) %>%
#   arrange(pc1) %>%
#   tibble() 
# 

rank_actual <- 
fantasy %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR) %>%
  tibble()

rank_pred <- 
fantasy %>%
  mutate(pc1 = (-.956*scoring_PassingYards)+(-.94*scoring_PassingTDs)+(.933*scoring_Interceptions)+
           (0.850*scoring_Fumbles)+(0.711*scoring_FumblesLost)) %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR,pc1) %>%
  arrange(pc1) %>%
  tibble() %>%
  rename("Model_OverallRank" = "Rank",
         "Actual_PositionRank"="PositionRank",
         "Model_Player"="Player",
         "Model_FantasyPointsPPR"="FantasyPointsPPR",
         "Model_PCscore"="pc1") %>%
  mutate(Model_PositionRank = c(1:82),.before = Model_Player) 

rank_diff <- cbind.data.frame(rank_actual[1:10,],rank_pred[1:10,-1])
knitr::kable(rank_diff)

```

The results are nearly identical to the previous method (on the original variables). Model accurately picks seven of the top ten players.

#### PC2 - WR

```{r}
pc_scoring <- principal(ffnorm_score,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") 

# fantasy %>%
#   mutate(pc2 = (0.958*scoring_Receptions)+(0.976*scoring_ReceivingYards)+(.923*scoring_ReceivingTDs)) %>%
#   filter(FantasyPosition=="WR") %>%
#   select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc2) %>%
#   arrange(desc(pc2)) %>%
#   tibble()


rank_actual <- 
fantasy %>%
  filter(FantasyPosition=="WR") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR) %>%
  tibble()

rank_pred <- 
fantasy %>%
  mutate(pc2 = (0.958*scoring_Receptions)+(0.976*scoring_ReceivingYards)+(.923*scoring_ReceivingTDs)) %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR,pc2) %>%
  arrange(desc(pc2)) %>%
  tibble() %>%
  rename("Model_OverallRank" = "Rank",
         "Actual_PositionRank"="PositionRank",
         "Model_Player"="Player",
         "Model_FantasyPointsPPR"="FantasyPointsPPR",
         "Model_PCscore"="pc2") %>%
  mutate(Model_PositionRank = c(1:575),.before = Model_Player)

rank_diff <- cbind.data.frame(rank_actual[1:10,],rank_pred[1:10,-1])
knitr::kable(rank_diff)

```

#### PC3 - RB 

```{r}
pc_scoring <- principal(ffnorm_score,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") 

# fantasy %>%
#   mutate(pc2 = (0.958*scoring_Receptions)+(0.976*scoring_ReceivingYards)+(.923*scoring_ReceivingTDs)) %>%
#   filter(FantasyPosition=="WR") %>%
#   select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc2) %>%
#   arrange(desc(pc2)) %>%
#   tibble() %>%
#   print(n=10)


rank_actual <- 
fantasy %>%
  filter(FantasyPosition=="WR") %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR) %>%
  tibble()

rank_pred <- 
fantasy %>%
  mutate(pc2 = (0.958*scoring_Receptions)+(0.976*scoring_ReceivingYards)+(.923*scoring_ReceivingTDs)) %>%
  select(Rank,PositionRank,Player,FantasyPointsPPR,pc2) %>%
  arrange(desc(pc2)) %>%
  tibble() %>%
  rename("Model_OverallRank" = "Rank",
         "Actual_PositionRank"="PositionRank",
         "Model_Player"="Player",
         "Model_FantasyPointsPPR"="FantasyPointsPPR",
         "Model_PCscore"="pc2") %>%
  mutate(Model_PositionRank = c(1:575),.before = Model_Player) 

rank_diff <- cbind.data.frame(rank_actual[1:10,],rank_pred[1:10,-1])
knitr::kable(rank_diff)

```

```{r, echo=FALSE, eval=FALSE,warning=FALSE,include=FALSE}

fviz_scre

fa.parallel(fantasy_cormatrix_scoring,n.iter=100,fa="pc", main="Parallel Scree")

# PA suggests 3 components

pc_scoring <- principal(ffnorm_score,nfactors=3,scores=TRUE,covar=FALSE,rotate="varimax") 

# RMSE = .99; cumulative explaind variance 88%; Mean item complexity is 1.1 as opposed to original 1.5 meaning interpretation is much easier as all variables load to only one component

print(pc_scoring$loadings,cutoff=0)

s <- prcomp(ffnorm_score,scale.=FALSE)

fviz_pca_biplot(s,
                    # Individuals
                    geom.ind = "point",
                    geom = c("arrow", "point"),
                    arrow.color = "black",
                    label = "var",
                    fill.ind = fantasy$FantasyPosition,
                    col.ind = "black",
                    pointshape = 21, 
                    pointsize = 1,
                    palette = "jco",
                    addEllipses = TRUE, 
                    repel = TRUE,
                    # Variables
                    alpha.var = "contrib", 
                    col.var = "black",
                    gradient.cols = "RdYlBu") +
  geom_rect(aes(xmin = -13.5, ymin = 0, xmax = -7.5, ymax = -3.5), 
            color = "grey30", 
            alpha = 0, 
            show.legend = FALSE) +  # Hide legend for this rect
  annotate("text", label = "Best Overall QBs", x = -12, y = -0.5, color = "grey30") +
  geom_rect(aes(xmin = 0.25, ymin = 1, xmax = 1, ymax = 1.75), 
            color = "grey30", 
            alpha = 0, 
            show.legend = FALSE) +  # Hide legend for this rect
  annotate("text", label = "Best Offensive \n long range players", x = -2.5, y = 2.5, color = "grey30") +
  geom_curve(aes(xend = 0.8, yend = 1.85, x = -2.8, y = 2.8), 
             color = "grey30", 
             arrow = arrow(), 
             curvature = -0.4, 
             show.legend = FALSE)  # Hide legend for this curve
    
fantasy %>%
  mutate(pc1 = (-.956*scoring_PassingYards)+(-.94*scoring_PassingTDs)+(.933*scoring_Interceptions)+
           (0.850*scoring_Fumbles)+(0.711*scoring_FumblesLost)) %>%
  filter(FantasyPosition=="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1) %>%
  arrange(pc1) %>%
  tibble() %>%
  print(n=20)


fantasy %>%
  mutate(pc2 = (0.958*scoring_Receptions)+(0.976*scoring_ReceivingYards)+(.923*scoring_ReceivingTDs)) %>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc2) %>%
  arrange(desc(pc2)) %>%
  tibble() %>%
  print(n=50) # couple TEs in the mix, 1 RB but mostly WRs as typical



fantasy %>%
  mutate(pc3 = (0.961*scoring_RushingYards)+(0.939*scoring_RushingTDs)) %>%
  filter(FantasyPosition!="QB") %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc3) %>%
  arrange(desc(pc3)) %>%
  tibble() %>%
  print(n=50)

# using all variables 

fantasy %>%
  mutate(pc1 = pc_og$scores[,1],
         pc2 = pc_og$scores[,2],
         pc3 = pc_og$scores[,3],
         ovr = pc1+pc2+pc3) %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1,pc2,pc3,ovr) %>%
  arrange(desc(ovr)) %>%
  #filter(FantasyPosition=="QB") %>%
  tibble() %>%
  print(n=20) %>%
  slice_min(FantasyPointsPPR,n=30) %>%
  summarize(sum(FantasyPointsPPR))

# BIPLOT


s <- prcomp(ffnorm_score,scale. = FALSE)

prcomp(ffnorm_score,scale.=FALSE)$val

# str(iris)
s$rotation[,c(2,3)]
# s <- data.frame(s$x)

# dim(s)
fviz_pca_biplot(s,
                    # Individuals
                    geom.ind = "point",
                    axes = c(3,2),
                    fill.ind = fantasy$FantasyPosition,
                    col.ind = "black",
                    pointshape = 21, pointsize = 1,
                    palette = "jco",
                    addEllipses = TRUE, 
                    # Variables
                    alpha.var ="contrib", col.var = NA,
                    gradient.cols = "RdYlBu")+geom_label("text",x=5,y=7,label="some text")


# BY POSITION 

fantasy %>%
  mutate(pc1 = pc_og$scores[,1],
         pc2 = pc_og$scores[,2],
         pc3 = pc_og$scores[,3],
         ovr = pc1+pc2+pc3) %>%
  select(Rank,PositionRank,FantasyPosition,Player,FantasyPointsPPR,pc1,pc2,pc3,ovr) %>%
  arrange(desc(ovr)) %>%
  tibble() %>%
  print(n=100)
  

# DRAFT ORDER QB > RB > WR > TE 

```



```{r, echo=FALSE, eval=FALSE,warning=FALSE,include=FALSE}
########################### 2023 NFL import and clean
fantasy2023 <- data.frame(read.xlsx("Fantasy Football Data/archive/fantasy_mergedsource_2023.xlsx"))
fantasy2023 %<>% mutate(Player=gsub("\\*","",Player),
                        Player=gsub("\\+","",Player))  

fantasy
fantasy2023 %>%
  select(Player) %>%
  tibble()
fantasy2023 %>%
  write.xlsx("fantasy2023.xlsx")
  
```



```{r, echo=FALSE, eval=FALSE,warning=FALSE,include=FALSE}
########################### 2024
fantasyprojection <- data.frame(read.xlsx("Fantasy Football Data/archive/fantasypointsprojections2024.xlsx"))
View(fantasyprojection)
```



# cont

```{r, echo=FALSE, eval=FALSE,warning=FALSE,include=FALSE}
fantasy %>%
  group_by(FantasyPosition) %>%
  filter(!is.na(ADP)) %>%
  slice_max(ADP, n = 3) %>%
  ggplot(aes(x=ADP,y=FantasyPointsPPR,shape=FantasyPosition))+
  geom_point() # potential cluster, k = 3

mean(fantasy$ADP,na.rm=TRUE)
```

```{r, echo=FALSE, eval=FALSE,warning=FALSE,include=FALSE}

# component space (pc1|pc2)

s <- prcomp(ffnorm_og,scale. = FALSE)

# str(iris)
# s <- data.frame(s$x)

# dim(s)
fviz_pca_biplot(s,
                    # Individuals
                    geom.ind = "point",
                    fill.ind = fantasy$FantasyPosition,
                    col.ind = "black",
                    pointshape = 21, pointsize = 1,
                    palette = "jco",
                    addEllipses = TRUE, 
                    # Variables
                    alpha.var ="contrib", col.var = NA,
                    gradient.cols = "RdYlBu")+annotate("text",x=5,y=7,label="some text")

# Iris dataset example
# res.pca <- prcomp(iris[, -5], scale. = TRUE)
# fviz_pca_ind(res.pca, geom.ind = "point", pointshape = 21, pointsize = 2, 
#              fill.ind = iris$Species, col.ind = "black", 
#              palette = "jco", addEllipses = TRUE, ellipse.level = 0.95)
```






