[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Independent Projects\nTomi Akisanya"
  },
  {
    "objectID": "ptsatisfaction.html#abstract",
    "href": "ptsatisfaction.html#abstract",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Abstract",
    "text": "Abstract\nPatient satisfaction can be an indication of quality care and aid in predicting health outcomes and patient retention. Although subjective, it is crucial for healthcare administrators to understand and meet patients’ expectations, translate it to patient-oriented care within delivery models, and improve population health as a result. This project investigates features associated with higher patient satisfaction scores for ER encounters and developing models to predict them."
  },
  {
    "objectID": "ptsatisfaction.html#data",
    "href": "ptsatisfaction.html#data",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Data",
    "text": "Data\nThis data set is a patient satisfaction survey administered to patients discharged from ten different entities at NMH over the span of two years. Respondents were discharged from six emergency units. Surveys were delivered either on paper or electronically following their encounter. The patient satisfaction surveys collected, range from discharges starting March of 2016 through January of 2018. Responses were given in either English or Spanish. Five types of payers were used for the encounters and satisfaction scores per response were given on a scale of one (very dissatisfied) to five (very satisfied). No other information on how the data was collected is available."
  },
  {
    "objectID": "ptsatisfaction.html#population-of-interest",
    "href": "ptsatisfaction.html#population-of-interest",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Population of Interest",
    "text": "Population of Interest\nThe population of interest is the patient population for emergency encounters at NM with disproportionate social determinants of health."
  },
  {
    "objectID": "ptsatisfaction.html#exclusion-criteria",
    "href": "ptsatisfaction.html#exclusion-criteria",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Exclusion Criteria",
    "text": "Exclusion Criteria\nA waterfall approach was used to define the population of interest. Social determinants, which include environmental and non-health related factors such as socio-economic status, can account for 30-55% of health outcomes1. Payer type was used as a proxy to define subsets of the population with disproportionate social determinants, with financial payers like Medicaid typically insuring low-income individuals. Self-pay and Medicaid payers were also included in the drop-down conditions, as this subset tends to misuse the ER for non-urgent care, serving as a viable proxy for lower health literacy.2. Patterns of ER misuse are also more likely to be younger and of non-hispanic black race, however, the only information available an race were responses indicating white or non-white. While sub-setting race to only non-whites would align more with the population of interest, this information was retained to maintain the basis of interpretation and prevent over specification of models in the event covariates are used. The age at which we thought individuals could best understand and articulate aspects of the emergency department setting was 16 years old. Records younger than that were omitted.\n\nwaterfall &lt;- mydata[mydata$disdate!=\"1900-01-01\" & mydata$survey_id!=1418129390,] #erroneous record; clearly incorrect entry \nwaterfall %&lt;&gt;% filter(age &gt;= 16, \n                      payor %in% c(\"Medicare\",\"Medicaid\",\"Self-Pay\")) #4603\n\n\nwaterfall %&lt;&gt;%\n  filter(!is.na(er_disch_time)) %&gt;%\n  filter(!is.na(er_admit_time))\n\nfor (i in seq_len(nrow(waterfall))){\n  if(sum(is.na(waterfall[i,]))&gt;8){\n    waterfall &lt;- waterfall[-i,]\n  }\n} #4101\n\nfor (i in seq_along(ncol((waterfall)))){\n  if(sum(is.na(waterfall[,i]))/4101&gt;.25){\n  #  print(names(mydata[,i]))\n    waterfall &lt;- waterfall[,-i]\n  }\n} \n\n# } #44 variable removed A87 / D2 / D52  = 4099 obs of 45 var\n\n\nwaterfall %&lt;&gt;%  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))\n\nwaterfall %&lt;&gt;% mutate(family_friends = (E1+E2+E3)/3,\n                      tests = (D4+D65+D3)/3,\n                      doctors = (C2+C4+C5+C75)/4,\n                      nurses = (B1+B3+B4+B5+B76)/5, \n                      arrival = (A4+A5+A86+C1)/4,\n                      personal_issues = (F1+F2+F41)/3,\n                      insurance = (A2+A3+A28)/3, \n                      overall = (E1+E2+E3+D4+D65+C1+C2+C4+C5+C75+B1+B3+B4+B5+B76+A4+A5+A86+F1+F2+F41+A2+A3+A28)/25)\n# additional variables will most likely remove due to multicollinearity\n\nwaterfall %&lt;&gt;% mutate(response_time = recdate - disdate, .before = distrib) # length of time b/w disch and receiving survey\n\nwaterfall %&lt;&gt;% filter(response_time&gt;=0) # filter out retroactive response times \n\ndim(waterfall) # final sample 2875 observations of 51 variables\n\n[1] 2875   54\n\n\nAfter cleaning records and additional formatting, observations were then cleaned. Survey ID 1418129390 had missing admit and discharge times and the encounter record for the survey was dated 1900-01-01. The implicit erroneous record was removed.\nTo maintain the integrity of the data set, observations with missing discharge or admit times were removed.\nObservations were subset to those over the age of 16 who were Medicare, Medicaid, or Self-Pay users to isolate the patient population. No demographic attributes were missing thereafter.\nRespondents were asked to survey their satisfaction across different care settings during their encounter. The Likert scale of possible responses were 1-5.\nFor the non-demographic survey questions, there were a number of missing responses. Survey respondents that had greater than 30% of answers missing were removed from the drop down conditions. Subsequently, survey questions across the sample that had more than 30% of responses missing were also omitted. The questions removed were:\nA87 (Arrival) - Helpfulness of person who first asked you about your condition D2 (Tests) - Courtesy of the person who took your blood D52 (Tests) - Concern shown for your comfort when your blood was drawn\nA new variable was created to measure the length of time between discharge date and the date the survey was received. Survey IDs with negative response times were present, implying that they received the survey before being admitted to the ER. These records were concluded as an error and were also omitted.\nSurvey responses could be grouped into one of the following care setting categories: Arrival, Personal/Insurance Info, Nurses, Doctors, Tests, Family or Friends, Personal Issues.\nFor any missing responses afterwards, the missing values were replaced with the average satisfaction score for that question.\n\nModel Validation Setup\n\nset.seed(555)\n\n# 70/30 test split \n\nwaterfall %&lt;&gt;% mutate(valid = runif(survey_id,0,1))\ntrain &lt;- waterfall %&gt;%\n  filter(valid &lt;= .7)\ntest &lt;- waterfall %&gt;%\n  filter(valid &gt; .7)\n\ndim(train)\n\n[1] 2049   55\n\ndim(test)\n\n[1] 826  55"
  },
  {
    "objectID": "ptsatisfaction.html#methods",
    "href": "ptsatisfaction.html#methods",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Methods",
    "text": "Methods\nA regression model was fit to predict the overall patient satisfaction variable, F68, and used to estimate the true population parameters. A 70/30 train:test ratio was used for model validation. Regression assumptions were then validated. For residual normality and increased model accuracy, explanatory variables were transformed before model fitting. The sampling distribution forms a left-skew distribution, in which the sixth root and natural log transformations were applied.\n\n\n\n\n\n\n\n\n\nObserving that the root transformation resulted in the greatest shift towards normality, it was applied to all explanatory variables. Additionally, summary variables based on category were calculated to enhance predictive power, specifically by computing the mean averages for each category. Several transformations were performed on the predictor variable F68 to improve the overall coefficient of determination and model fit. Consequently, the final explanatory parameters are based on the root of F68.\nPearson’s product correlation matrix was used for variable selection; starting with the highest coefficient of determination, a regression model was fit in progressive steps with one or two variables at a time. At each step, a combination of diagnostic tests and empirical thresholds were used to assess goodness-of-fit based on the following criteria:\n\\[ Δ R^2 = R_f^2 - R_n^2 &gt; .2 \\]\nWhere the additional variable must increase the coefficient of determination by 2% in order for the variable to be retained.\nTurkey’s Nonadditive Test was used to asses presence of interaction. In the context of regression, fitted values squared are computed post-hoc as a quadratic function to test if the interaction term is significantly different from zero, assuming H0: ŷ = 0 and a linear function is modeled with Ha: ŷ ≠ 0 and a non-linear function is modeled:\n\\[\\hat{f_n} = (β_1)\\hat{x_1} + (β_2)\\hat{x_2}+ (β_3)\\hat{x_3}...+(\\hat{y_n})^2\\hat{x_z}\\]\n\\[\\hat{y_n} = (β_1)\\hat{x_1} + (β_2)\\hat{x_2}...+(β_z)\\hat{x_z}\\]\nwhere ŷf are the full predicted values and ŷn are the nested predicted values, variables were retained if P(F) &lt; .05.\nDifference in fits was used to measure the influence of individual observations obtained using an empirical threshold of \\({\\small √(p)/n}\\); where p is the number of parameters and n is the number of observations\nCook’s distance uses leverage and studentized residuals to measure significance of observation on overall model; threshold of 4/n was considered to be practically influential and was further evaluated\nWald Statistic: {\\(β^2/Var(β)\\)} measures the effect size of individual parameters.\nVariance Inflation Factors greater than four were reevaluated; greater than seven are removed.\nThe model yielding the optimal Δ R2 and least RMSE was used as the production model. The questions used to guide the final model were:\n\nDoes the model capture the true population parameters and relationship? Can the model be used to draw generalizations from our target population?\nIs the model an accurate predictor of patient satisfaction?\n\nThe results of the diagnostic post-hoc tests were then used to assess the models capability to generalize to the target population and decide if a different model would be more equipped to capture the relationships found in survey responses to overall patient satisfaction in ER encounters for those with low social determinants of health. In practice, it is acknowledged that no generalizations can accurately be made on our target population due to the limited information around how the data was collected. It is unknown if the observations were from a random sample, which is sufficient and necessary to make any population conclusions. Any aforementioned hypothesis to the population is for the sake of statistical inference and this endeavor. However, to validate prediction capabilities, a precision grade was used on our test set."
  },
  {
    "objectID": "ptsatisfaction.html#analysis",
    "href": "ptsatisfaction.html#analysis",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Analysis",
    "text": "Analysis\nPearson’s product correlation plot showed that overall satisfaction was strongly correlated with how well patients were informed about delays and other responses based on either doctors’ or nurses’ care setting.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nCorrelation\nCategory\nQuestion\n\n\n\n\nF2\n0.796\nPersonal Issues\nHow well you were kept informed about delays\n\n\nB3\n0.734\nNurses\nNurses’ attention to your needs\n\n\nB4\n0.733\nNurses\nNurses’ concern to keep you informed about your treatment\n\n\nC4\n0.727\nNurses\nDoctor’s concern for your comfort while treating you\n\n\nC2\n0.674\nDoctors\nCourtesy of the doctor\n\n\nB5\n0.655\nDoctors\nNurses’ concern for your privacy\n\n\nC1\n0.629\nDoctors\nWaiting time in the treatment area, before you were seen by a doctor\n\n\n\n\n\nF2 was therefore used as the initial predictive variable and subsequent variables were added in a ‘stepwise’ fashion while assessing the change in R2 and MSE. Variables within the same category exhibited a high degree of underlying collinearity and summary statistics were calculated for the means of the top two responses or the total responses per group to remediate covariance. This is preferable since 1) information can be easily summarized into one parameter instead of multiple, decreasing the Alkaline Information Criterion while maintaining parsimony and 2) missing responses are a notable characteristic of survey data and limiting the model to individual parameters can lead to inaccuracy for future data sets that have large proportions of missing responses to individual questions.\n\ntrain %&lt;&gt;%\n  mutate(across(c(16:46), ~ (.)^(1/6), .names = \"{col}_rt\"), .before = family_friends) \n            \ntrain %&lt;&gt;%\n  mutate(arrival_2 = (A5_rt+C1_rt)/2, doctors_2 = (C4_rt+C75_rt)/2,\n         family_friends_2 = (E1_rt+E3_rt)/2, nurses_2 = (B4_rt+B3_rt)/2,\n         personal_issues_2 = (F2_rt+F41_rt)/2, test_2 = (D3_rt+D4_rt)/2) #averages post-trans\n\ntrain %&lt;&gt;%\n  mutate(arrival_3 = (A5_rt+C1_rt+A86_rt)/3, doctors_3 = (C4_rt+C75_rt+C5_rt)/3,\n         family_friends_3 = (E1_rt+E2_rt+E3_rt)/3, nurses_3 = (B4_rt+B3_rt+B76_rt)/3,\n         personal_issues_3 = (F2_rt+F41_rt+F1_rt)/3, test_3 = (D3_rt+D4_rt+D65_rt)/3) #averages post-trans\n\ntrain %&lt;&gt;% mutate(personal_issues_3_rt = personal_issues^(1/6),\n                  F68_rt = sqrt(F68))"
  },
  {
    "objectID": "ptsatisfaction.html#results",
    "href": "ptsatisfaction.html#results",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Results",
    "text": "Results\nThe final regression is modeled by\n\\[ f(x) = -3.52083 + 1.94361*x_1 + 1.35390*x_2 + 1.11454*x_3 \\] \\[ RMSE = .12 \\]\nwhere β1 describes change in ŷ for a one unit change in the root mean satisfaction for the personal issues category, β2 for a one unit change in the root mean satisfaction for the nurses category, and β3 for a one unit change in the root indicator for Doctor’s concern for patients comfort while treating them. Interpretations for β0 is omitted due to transformations and range of the data set. The Adjusted R2 ~ .74, F(3,2045), p = 2.2e-16 using adjusted Type I error rate = .005.\nOur model produced a MSE = 0.015 (RMSE = 0.123) with statistically significant results for all predictors. The RMSE quantifies the models predictive capability in the context of regression such that it measures the average difference between the observed values and the predicted values of our model. In other words, the model predicts overall satisfaction within .12 of the actual satisfaction scores for the training data set. Although robust and promising, true confirmation of performance is assessed on the training data set via precision grades below.\n\nmodel &lt;- lm(F68_rt~personal_issues_3_rt+nurses_3+C4_rt, data = train)\nanova(model)\n\nAnalysis of Variance Table\n\nResponse: F68_rt\n                       Df Sum Sq Mean Sq F value    Pr(&gt;F)    \npersonal_issues_3_rt    1 76.254  76.254 5196.77 &lt; 2.2e-16 ***\nnurses_3                1  5.688   5.688  387.64 &lt; 2.2e-16 ***\nC4_rt                   1  3.280   3.280  223.52 &lt; 2.2e-16 ***\nResiduals            2045 30.007   0.015                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf the data was obtained via random sampling, the model then could be used as an estimate for the true target population’s overall satisfaction. Residual plots were used to test residual normality and linearity with P(F) = .57 on one degree of freedom for Tukey’s test for non-additivity. Residual values lie within three standard deviations with a few minor exceptions in the fitted values, prompting an opportunity for different optimization methods. The apparent uncaptured variation between the predictor variables calls for models suited for continuous ordinal data such as logistic, poisson, or generalized linear models that can capture different types of distributions.\n\nresidualPlots(model)\n\n\n\n\n\n\n\n\n                     Test stat Pr(&gt;|Test stat|)\npersonal_issues_3_rt   -1.3799           0.1678\nnurses_3                0.6213           0.5345\nC4_rt                  -0.5763           0.5645\nTukey test              0.3667           0.7138\n\n\nSurvey IDs 1344260683 and 1439794901 had the highest influence on the overall model in comparison to the global average of Cook’s distance. Looking at the original data, the former had missing responses for four of the seven questions used to produce the model, highlighting a major flaw in this model - its predictive inaccuracy for patients with missing responses - as missing responses were filled with grand means after exclusion criteria were applied. Although missing at random, a better approach would be to fill in with median values or apply more conservative exclusion factors such as omitting observations with greater than 10% of responses missing. The latter was greatly considered, but the trade-off of small sample sizes and subsequent over fitting were outweighed.\nAfter confirming the absence of variance inflation, the Log Likelihood ratio test was done for the nested model excluding x3, as this was the only non-summary predictor. Wald statistics are limited for linear models such that P(Wald) &gt; P(LRT). Assuming null hypothesis is true where the nested model is just as adequate as the full model, p(χ) = 1.2e-48 (215,1) the affect of x3 on the model is statistically significant.\n\nmodelx &lt;- lm(F68_rt~personal_issues_3+nurses_3, data = train)\n-2*(logLik(modelx)-logLik(model)) # 5455.716, df = 4\n\n'log Lik.' 214.9064 (df=4)\n\npchisq(214.9064,1,lower.tail = FALSE)\n\n[1] 1.168115e-48"
  },
  {
    "objectID": "ptsatisfaction.html#footnotes",
    "href": "ptsatisfaction.html#footnotes",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWilliams JN, Drenkard C, Lim SS. The impact of social determinants of health on the presentation, management and outcomes of systemic lupus erythematosus. Rheumatology (Oxford). 2023 Mar 29;62(Suppl 1):i10-i14. doi: 10.1093/rheumatology/keac613. PMID: 36987604; PMCID: PMC10050938.↩︎\nNaouri D, Ranchon G, Vuagnat A, Schmidt J, El Khoury C, Yordanov Y; French Society of Emergency Medicine. Factors associated with inappropriate use of emergency departments: findings from a cross-sectional national study in France. BMJ Qual Saf. 2020 Jun;29(6):449-464. doi: 10.1136/bmjqs-2019-009396. Epub 2019 Oct 30. PMID: 31666304; PMCID: PMC7323738.↩︎"
  },
  {
    "objectID": "ffsgvmedicare.html",
    "href": "ffsgvmedicare.html",
    "title": "Identifying High-Need High Cost Patients within the Fee-For-Service Medicare Benefeciaries Population - Part 1",
    "section": "",
    "text": "Amount and quality of health services received vary substantially across different regions and subgroups within. The cause of this variation does not appear to be a result of differences in social-determinants, and estimates show that 33% of expenditures are unnecessary nor improve population health1. Moreover, in 2021, 5% of the population accounted for nearly half (48.4%) of total health expenditures in the United States2. High-Need High-Cost Patients have been the focus of utilization tracking by all stakeholders. There has been no formal definition to this population, but it is understood by most health experts that 1) they represent a small subset of the population and 2) disproportionately have higher spending than their counterparts. 3The ability to clearly define this subset of the population, observe their health needs, and analyze differences in utilization, spending, and quality of care across various factors and geographic regions can provide additional insight to US health expenditures, drive actionable change, and produce targeted initiatives. The Fee for Service Geographic Variation User File was created by the CMS in efforts to explain the underlying variation in resource use among Medicare beneficiaries across regions and physician practice patterns. It is foundational on CMS’ Chronic Conditions Data Warehouse (CCW), a data set containing 100% of Medicare beneficiaries’ enrolled in the fee-for-service claims as well as other enrollment types from 2014-2022. For this reason, it is acknowledged that this is census data. Therefore, generalizations in the form of hypothesis testing and statistical inferences are not used in this analysis. Any observable differences in the graphics or summary statistics have been concluded as differences in the population; standard error(s) measurements have been subsequently omitted."
  },
  {
    "objectID": "ffsgvmedicare.html#footnotes",
    "href": "ffsgvmedicare.html#footnotes",
    "title": "Identifying High-Need High Cost Patients within the Fee-For-Service Medicare Benefeciaries Population - Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Eliminating Waste in US Healthcare,” by Donald M. Berwick and Andrew D. Hackbarth, 2012↩︎\nConcentration of Healthcare Expenditures and Selected Characteristics of Persons with High Expenses, United States Civilian Noninstitutionalized Population, 2018-2021, meps.ahrq.gov/data_files/publications/st556/stat556.shtml#:~:text=Persons%20in%20the%20top%205%20percent%20expenditure%20tier%20accounted%20for,97.2)%20of%20total%20healthcare%20expenses. Accessed 16 July 2024. ↩︎\nNational Academies of Sciences, Engineering, and Medicine; Health and Medicine Division; Board on Population Health and Public Health Practice; Committee on Community-Based Solutions to Promote Health Equity in the United States; Baciu A, Negussie Y, Geller A, et al., editors. Communities in Action: Pathways to Health Equity. Washington (DC): National Academies Press (US); 2017 Jan 11. 2, The State of Health Disparities in the United States. Available from: https://www.ncbi.nlm.nih.gov/books/NBK425844/↩︎\nAHRQ Quality IndicatorsTM (AHRQ QITM) ICD-10-CM/PCS Specification v2022↩︎\nSuper N. The Geography of Medicare: Explaining Differences in Payment and Costs [Internet]. Washington (DC): National Health Policy Forum; 2003 Jul 3. (Issue Brief, No. 792.) Available from: https://www.ncbi.nlm.nih.gov/books/NBK559778/↩︎\nNational Academy of Medicine; The Learning Health System Series; Whicher D, Dahlberg ML, Apton KL, et al., editors. Effective Care for High-Need Patients: Opportunities for Improving Outcomes, Value, and Health. Washington (DC): National Academies Press (US); 2017. 2, KEY CHARACTERISTICS OF HIGH-NEED PATIENTS.↩︎\nPercent increase is relative to data set: 2014-2022↩︎\nMitchell, E. M. Concentration of Healthcare Expenditures and Selected Characteristics of Persons with High Expenses, U.S. Civilian Noninstitutionalized Population, 2018. Statistical Brief #533. January 2021. Agency for Healthcare Research and Quality, Rockville, MD. \nhttp://www.meps.ahrq.gov/mepsweb/data_files/publications/st533/stat533.shtml↩︎\nBorksy, A., et al. (2018). Few Americans Receive All High-Priority, Appropriate Clinical Preventive Services. Health Affairs, 37(6). DOI: 10.1377/hlthaff.2017.1248↩︎"
  },
  {
    "objectID": "ptsatisfaction.html#model-validation",
    "href": "ptsatisfaction.html#model-validation",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Model Validation",
    "text": "Model Validation\nThe challenge with model validation is being conscious of the present data and methods taken throughout the modeling process on the test set and appropriately applying them chronologically on the test set. Inefficient processes/workflows used in the modeling phase will often reveal itself in testing. The variance-bias tradeoff is the central phenomena that typically only be visualized during testing. While there are preventative measures analysts take initially to prevent this such as model specification and applied weights, benchmarks need to be created to measure the ability to be precise and to maintain that precision on unseen data.\nAfter using the parameters from the trained model to calculate the predicted satisfaction scores of our test set, the Mean Absolute Error (MAE) was used to calculate percent accuracy for the test. Arbitrary cutoff points where then assigned four grades ranging from letters A through F- given the following criteria:\n\nVariance/Bias Tradeoff\nThe grades are only one of many test diagnostics used to measure how well a given model fits a subset of data (model fit) and how well it fits unseen data. The risk in the former is fitting a model too well. There are two common scenarios that both are operational and are forms overfitting and underfitting. The first being that the trained model fits very well on the initial data. These models will exhibit properties that are ‘too good to be true’ such as an RMSE ~ 0 or a predictive accuracy ~100%. The issue then becomes a lack of flexibility - mainly components of lack of parsimony, very few observations, or unrecognized bias in data (not to be confused with modeling bias). These models often perform very poorly on new data. With regard to the latter, underfitting is less common and exhibits the converse process. Upon initial assessment, the grades look promising. The absolute value of the difference between the predicted and observed values are reported as proportions and easier to interpret below. The model predicted satisfaction score for each patient is within 91% of the actual score. 1.9% of patient satisfaction scores were off by greater than 25% on the polar opposite (F -). Visualizing the models outputs reveals the ostensible performance of the model.\n\n# Prediction Grade\nforward.pct &lt;- abs(test$F68_rt-fittedtest)/test$F68_rt;\n\nforward.PredictionGrade &lt;- ifelse(forward.pct&lt;=0.10,'Grade A: [0,0.10]',\nifelse(forward.pct&lt;=0.15,'Grade B: (0.10,0.15]',\nifelse(forward.pct&lt;=0.25,'Grade C: (0.15,0.25]',\n'Grade F-: (0.25+]')))\nforward.Table &lt;- table(forward.PredictionGrade)\nforward.Table/sum(forward.Table)\n\nforward.PredictionGrade\n   Grade A: [0,0.10] Grade B: (0.10,0.15] Grade C: (0.15,0.25] \n          0.91767554           0.03753027           0.02542373 \n   Grade F-: (0.25+] \n          0.01937046 \n\n\n\nPredicted vs Residuals\nThe first plot is a density bin plot where the x axis are the predicted values and the y axis are the residuals or how much the model was off by. Note that these units are not to scale with the original patient satisfaction scores since the data was transformed for model compatibility. The red line indicates where the error (and data) should be centered. Robust models, and the goal of optimization in the OLS regression, is to fit a line on a cloud of data such that the residual error is zero (if predicted and actual values are the same, the model is 100% accurate and residuals equal zero). This is idealistic and impractical, but nevertheless is what’s being optimized. The red line sits beneath all data points, indicative of severe over fitting. For all patients, the model over predicts patient satisfaction and most likely will for future data sets. The fully optimized estimated regression line most likely runs somewhere where the yellow line is. Not only would the model be more balanced in regard to variance-bias, residual normality is the underlying assumption for OLS normality.\n\n\n\n\n\n\n\n\n\n\n\nAbsolute Error\nThe produced residuals from testing form a right skew log-normal distribution. During wrangling, the sampling distribution had an identical distribution but was skewed left. The severe deviations from a centered distribution with relatively even kurtosis may result in long-term predictive inaccuracies if not remedied. The last visualization shares similarities with influence visualizations. The x-axis is indexed to the error on the y-axis, allowing for comparisons of individual residual errors for each observation across the test data set. The indexes with taller heights can be an implication of the observations individual impact to the model. Models with balanced variance-bias should have uniform height.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted vs Observed\nResidual values against predictions can reveal the presence of normality. If the model performs at a 100% accuracy, the predicted and observed values should be 1:1 and the points should subsequently form a regression line with as close to a slope of one as possible. Granted, this is more difficult to accomplish given the nature of the data set. Patient satisfaction scores are discrete values that take whole numbers from 1-5 in the original data set, the reason there are ~5 discrete lines that are shown below."
  },
  {
    "objectID": "ptsatisfaction.html#conclusion",
    "href": "ptsatisfaction.html#conclusion",
    "title": "Forecasting Patient Satisfaction for Emergency Department Encounters",
    "section": "Conclusion",
    "text": "Conclusion\nThe ability to quantify and understand the relationships between different hospital staff interactions during emergency care encounters can be powerful for creating quality and retention initiatives within communities with lower social determinants of health. The model revealed that key predictors of overall patient satisfaction included the doctor’s attention to the patient’s care, communication about delays, the degree to which staff demonstrated care for the patient as an individual, how well pain was controlled, and the attention and courtesy provided by nurses. By focusing on these critical aspects of care, especially in patient populations like Medicaid, Medicare, and self-pay patients, hospitals can implement targeted quality improvement initiatives that address the specific needs of these groups. Enhancing these areas not only improves patient satisfaction but also promotes better retention and long-term health outcomes. This data-driven approach enables healthcare providers to offer more personalized, effective care, ultimately fostering a more equitable and satisfying patient experience in communities with significant social and economic challenges."
  },
  {
    "objectID": "Fantasy Football.html",
    "href": "Fantasy Football.html",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "",
    "text": "Around this time every year, if you’re like me, you’ve already started to mentally prepare for 80% of your conversations to be on the topic of football. For those individuals that would not classify themselves as ffanatics, it’s probably annoying - it’s annoying for all of us. Pretty consistently there will behavior from fans that range from DMs to athletes, verbal assaults to close friends, damaged property, and public humiliation. Who realistically has the mental endurance to only discuss a single topic over an extended period of time for something they’re not physically involved in? It’s those with passion, and although I am not passionate about football (nor justify the aforementioned behavior), it will be my first of hopefully many Fantasy Football leagues. If there’s one thing I am passionate about, it’s tilting the odds in my favor. Usually that’s in the form of taking a creative line on the felt on a 2/5 reg by extracting max value with 67s on a low connected board in a 4! pot as the preflop aggressor. Balance and discipline is the name of the game though, and who are we if we don’t apply the same approach to all areas in life…"
  },
  {
    "objectID": "Fantasy Football.html#data-exploration",
    "href": "Fantasy Football.html#data-exploration",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Data Exploration",
    "text": "Data Exploration\nFactors such as free agency, transfers, injuries, and retiring players drastically change the NFL landscape and subsequent fantasy performance for later seasons, so it is important to look at the most current season if possible. After subsetting the data for the most recent season, the data set was skimmed for a review of distributions, counts, and other elements within the data set.\n\n\n\nData summary\n\n\nName\nfantasy\n\n\nNumber of rows\n575\n\n\nNumber of columns\n43\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n2\n\n\nnumeric\n39\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nPlayer\n0\n1\n8\n24\n0\n575\n0\n\n\nPlayerID\n0\n1\n8\n8\n0\n575\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nTeam\n0\n1\nFALSE\n34\n2TM: 28, ARI: 20, DEN: 20, LAC: 20\n\n\nFantasyPosition\n0\n1\nFALSE\n4\nWR: 218, RB: 162, TE: 113, QB: 82\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nRank\n0\n1.00\n289.17\n166.96\n1.00\n144.50\n290.00\n433.50\n577.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n26.19\n3.23\n21.00\n24.00\n26.00\n28.00\n45.00\n▇▇▁▁▁\n\n\nGames\n0\n1.00\n11.60\n5.16\n1.00\n8.00\n13.00\n16.00\n17.00\n▂▂▂▂▇\n\n\nGamesStarted\n0\n1.00\n5.56\n5.81\n0.00\n0.00\n3.00\n11.00\n17.00\n▇▂▂▂▂\n\n\nCompletions\n0\n1.00\n19.98\n72.11\n0.00\n0.00\n0.00\n0.00\n490.00\n▇▁▁▁▁\n\n\nAttempts\n0\n1.00\n31.08\n110.35\n0.00\n0.00\n0.00\n0.00\n733.00\n▇▁▁▁▁\n\n\nPassingYards\n0\n1.00\n219.00\n789.12\n0.00\n0.00\n0.00\n0.00\n5250.00\n▇▁▁▁▁\n\n\nPassingTDs\n0\n1.00\n1.30\n5.04\n0.00\n0.00\n0.00\n0.00\n41.00\n▇▁▁▁▁\n\n\nInterceptions\n0\n1.00\n0.72\n2.40\n0.00\n0.00\n0.00\n0.00\n15.00\n▇▁▁▁▁\n\n\nRushingAttempts\n0\n1.00\n25.65\n56.65\n0.00\n0.00\n2.00\n17.00\n349.00\n▇▁▁▁▁\n\n\nRushingYards\n0\n1.00\n114.51\n261.73\n-15.00\n0.00\n5.00\n73.50\n1653.00\n▇▁▁▁▁\n\n\nYardsPerAttempt\n0\n1.00\n2.62\n3.69\n-7.50\n0.00\n1.75\n4.46\n40.00\n▇▇▁▁▁\n\n\nRushingTDs\n0\n1.00\n0.85\n2.14\n0.00\n0.00\n0.00\n1.00\n17.00\n▇▁▁▁▁\n\n\nTargets\n0\n1.00\n29.95\n37.05\n0.00\n3.00\n14.00\n44.00\n184.00\n▇▂▁▁▁\n\n\nReceptions\n0\n1.00\n20.11\n24.93\n0.00\n2.00\n9.00\n30.00\n128.00\n▇▂▁▁▁\n\n\nReceivingYards\n0\n1.00\n220.03\n304.02\n-10.00\n11.00\n96.00\n313.50\n1809.00\n▇▂▁▁▁\n\n\nYardsPerReception\n0\n1.00\n8.67\n6.23\n-6.00\n5.00\n8.90\n12.11\n42.00\n▃▇▂▁▁\n\n\nReceivingTDs\n0\n1.00\n1.30\n2.12\n0.00\n0.00\n0.00\n2.00\n14.00\n▇▁▁▁▁\n\n\nFumbles\n0\n1.00\n1.04\n1.98\n0.00\n0.00\n0.00\n1.00\n16.00\n▇▁▁▁▁\n\n\nFumblesLost\n0\n1.00\n0.48\n0.98\n0.00\n0.00\n0.00\n1.00\n9.00\n▇▁▁▁▁\n\n\nFantasyPointsPPR\n0\n1.00\n78.41\n85.43\n-2.90\n11.50\n43.40\n116.35\n417.40\n▇▂▂▁▁\n\n\nPositionRank\n0\n1.00\n81.82\n55.22\n1.00\n37.00\n73.00\n118.50\n218.00\n▇▇▅▃▂\n\n\nYear\n0\n1.00\n2022.00\n0.00\n2022.00\n2022.00\n2022.00\n2022.00\n2022.00\n▁▁▇▁▁\n\n\nADP\n429\n0.25\n71.90\n42.07\n1.30\n36.42\n70.65\n107.15\n153.80\n▇▇▇▇▅\n\n\nCompletionsPerAttempt\n0\n1.00\n0.10\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nTDsPerAttempt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nInterceptionsPerAttempt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nTDsPerReception\n0\n1.00\n0.05\n0.11\n0.00\n0.00\n0.00\n0.07\n1.00\n▇▁▁▁▁\n\n\nFumblesLostPerFumble\n0\n1.00\n0.20\n0.36\n0.00\n0.00\n0.00\n0.25\n1.00\n▇▁▁▁▂\n\n\nscoring_PassingYards\n0\n1.00\n8.76\n31.56\n0.00\n0.00\n0.00\n0.00\n210.00\n▇▁▁▁▁\n\n\nscoring_PassingTDs\n0\n1.00\n5.18\n20.16\n0.00\n0.00\n0.00\n0.00\n164.00\n▇▁▁▁▁\n\n\nscoring_Interceptions\n0\n1.00\n-1.44\n4.80\n-30.00\n0.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\nscoring_RushingYards\n0\n1.00\n1.15\n2.62\n-0.15\n0.00\n0.05\n0.74\n16.53\n▇▁▁▁▁\n\n\nscoring_RushingTDs\n0\n1.00\n5.08\n12.82\n0.00\n0.00\n0.00\n6.00\n102.00\n▇▁▁▁▁\n\n\nscoring_Receptions\n0\n1.00\n20.11\n24.93\n0.00\n2.00\n9.00\n30.00\n128.00\n▇▂▁▁▁\n\n\nscoring_ReceivingYards\n0\n1.00\n22.00\n30.40\n-1.00\n1.10\n9.60\n31.35\n180.90\n▇▂▁▁▁\n\n\nscoring_ReceivingTDs\n0\n1.00\n7.79\n12.72\n0.00\n0.00\n0.00\n12.00\n84.00\n▇▁▁▁▁\n\n\nscoring_Fumbles\n0\n1.00\n-2.09\n3.96\n-32.00\n-2.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\nscoring_FumblesLost\n0\n1.00\n-0.96\n1.96\n-18.00\n-2.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\n\n\n\nImportant categorical variables to note outside of the Team and Player is the Fantasy Position. These are QBs, WRs, TEs, and RBs. All defensive positions are not scoped and have been explicitly removed from the data set. The objective of this project is to create a fantasy team that has the highest likelihood of obtaining the most Fantasy Points (PPR) for the upcoming season based on prior seasons. The next logical question becomes, does this likelihood vary by team? By position? Intuitively, the likelihood will vary simply based on how many of these positions are in game at a given time. A table of each position shows the distribution of each position in a given NFL season.\n\n\n FantasyPosition   n   percent\n              WR 218 0.3791304\n              RB 162 0.2817391\n              TE 113 0.1965217\n              QB  82 0.1426087\n\n\nA majority of NFL players in 2022 for fantasy purposes were wide receivers (38%) while the least common position were quarterbacks (14%). For the continuous variables, the important things to note are the means, standard deviations, their counts and distribution via the histograms, and any missing values. In-game stats such as Yards Per Attmept, Yards per Reception, and additional variables (below ADP) are missing for a lot of players as expected. The missing values are likely due to a number of factors such as the position of the player and the number of games each played. The range of games played by each player vary from none to 17, representative of 17 total games in the regular season. The descriptive summary for Fantasy Points (PPR) is:\n\n\n   vars   n  mean    sd median trimmed   mad  min   max range skew kurtosis\nX1    1 575 78.41 85.43   43.4      64 57.08 -2.9 417.4 420.3 1.38      1.4\n     se\nX1 3.56\n\n\nWith mu = 78.4 (sd = 85.29), it is obvious that there is high variability. The histogram of PPR shows where how this variability is distributed.\n\n\n\n\n\n\n\n\n\nDepending on the fantasy league, PPR scoring will be different in the sense that there is typically a PPR threshold per game for a player in order for their PPR to be recorded for that given week. If the player is below the baseline, the PPR may be zero. Interestingly enough, the distribution of PPR in 2022 is a log-normal right skew distribution. The observed PPR for most players were approximately zero. This is logical, granted the extreme difficulty of being a top NFL performer. Most players are benched, aren’t playing games, nor are starters. Evidently, most of the continuous variables will follow the same trend with observed frequencies ~0 and relatively a fewer number of players scoring the most for each variable. There are three notable exceptions:\n\n\n\n\n\n\n\n\n\n\nAverage Draft Position - Uniform Distribution: For players that have this data available, ADP is uniformly distributed across most players. This implies that the parent population drafts players evenly across the board and there isn’t a strong concentration of players being picked predominantly. To reiterate, this is specifically for players that have ADP data available, which may be a combination of the most popular or the best players.\nFumbles Lost per Fumble - Bimodal distribution: Frequency of Fumbles Lost per Fumble have peaks at both zero and one, revealing that most players either fumble and lose possession of the ball, resulting in a turnover, or don’t fumble at all.\nYards Per Reception - Normal distribution: Players average 10 Yards Per Reception with a majority of players within the range of two standard deviations from the mean. Although there is a slight right skew, this is one of the few variables that follows a Gaussian distribution.\n\nThe first thing we want to understand is what position, if any, we should be more inclined to draft first, and their likelihood of obtaining the most fantasy points. I started by seeing what the public likes to draft first. In 2022, the public drafted quarterbacks around 86 times on average, the highest of the four positions, with tight ends at a close second of 84 times on average. Running backs and wide receivers then followed. Acknowledging that ADP data is not available for all positions, I wanted to better understand if quarterbacks are the biggest factor in regards to PPR. A comparison of ADP and PPR by position begins to paint the picture. On average, quarterbacks had 105 fantasy points in 22, followed by wide receivers, running backs, then tight ends.\n\n\n# A tibble: 4 × 3\n  FantasyPosition AverageDraftPosition AveragePPR\n  &lt;fct&gt;                          &lt;dbl&gt;      &lt;dbl&gt;\n1 QB                              85.8      105. \n2 WR                              73.5       81.6\n3 RB                              61.5       75.7\n4 TE                              83.9       56.5\n\n\n# A tibble: 4 × 2\n  FantasyPosition     n\n  &lt;fct&gt;           &lt;dbl&gt;\n1 QB               85.8\n2 RB               61.5\n3 TE               83.9\n4 WR               73.5\n\n\nThis seems to be consistent at a higher level when the top three teams for the 2022 regular season are compared.\n\n\n`summarise()` has grouped output by 'Team'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nHowever, properly accounting for total number of games played by player, yields different results. Evidently, quarterbacks are disproportionately the most efficient in regard to fantasy points on a per game basis at .164 per game. Running backs, tight ends, and receivers consecutively follow but are significantly behind. However, fantasy points between those three positions vary by less than 5%. The logical question becomes what are the underlying causes of this variation? One possibility is that there are typically more wide receivers on an offensive play than any other position. Another possibility can be due to the extreme routes and distances wide receivers run, making them injury prone and therefore less efficient.\n\n\n# A tibble: 4 × 2\n  FantasyPosition PPRperGame\n  &lt;fct&gt;                &lt;dbl&gt;\n1 QB                  0.164 \n2 RB                  0.0393\n3 TE                  0.0372\n4 WR                  0.0316"
  },
  {
    "objectID": "Fantasy Football.html#analysis-svd-pca",
    "href": "Fantasy Football.html#analysis-svd-pca",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Analysis SVD PCA",
    "text": "Analysis SVD PCA\nEigenvalue decomposition is an unsupervised machine learning method typically used for dimensionality reduction on mostly unlabeled data. The same approach is used here, with the intent to reduce the data to components that measure performance in some aspect. The best way to think about this is in the form of a recipe. A cookbook’s recipe for chicken cordon bleu will have elaborate concoctions and mixes of different food. When applied in this context, it would reduce the recipe to its core components, 3/4 chicken, 1/8 cheese, 1/8 ham let’s say. It then becomes much easier to make chicken cordon bleu while keeping most of the taste. While an oversimplification, the approach is essentially the same, with the goal of maximizing the amount of underlying variation using linear combinations of variables. At its core, there is some latent underlying variable(s) that combinations of these variables measure. What those underlying variables measure and its relevancy is on us to define. These are the principal components. The principal components are made up of the original variables, and how much that variable contributes (or is a proportion of) to the underlying variable (ie principal component) are the eigen vectors or loadings. Loadings can be positive (greatly contributes) or negative (adversely contributes). How well the variables load help define what that new underlying variable is. To define the inclusion criteria, any variable that loads +/- .7 will be considered as loading well and those variables alone will be what is used to define the underlying variable/component.\nA parallel test was used to measure the number of components to obtain. The test performs the same decomposition on simulated data of the same size and graphs the results. Where the simulated and actual data intersect is the cutoff for the number of components to obtain. The results of the test suggest three components. The y-axis plots the eigenvalues which is the total variation explained by each component. In a simpler sense, it can be thought of as the number of original variables accounted for in that component (hence the horizontal line separating values less than one). Three principal components were obtained, acknowledging that principal component one (PC1) should account for approximately six variables, PC2 around 5, and PC3 around 3. The other scree plot better highlights the components as a percentage of total variability explained. Keep in mind that PC1 only accounts for 30% of the total variability and the first three components cumulatively account for 52% of total variability. It’s likely that the post-hoc tests described in the methods section will not be sufficient for our goal since there is still half of the total variation not accounted for in these components.\n\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  3 \n\n\n\n\n\n\n\n\n\n\\[ A * v =  λ * v \\]"
  },
  {
    "objectID": "Fantasy Football.html#principal-components",
    "href": "Fantasy Football.html#principal-components",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Principal Components",
    "text": "Principal Components\nrotation: maximum variance\nPC1 - Definition: High-Volume QBs Attributes: Completions, Attempts, PassingYards, PassingTDs, Interceptions, Fumbles\n\nPC1 can be attributed to high-volume QBs as they load extremely well for the above categories. Although pc2 and pc3 are components attributed to the skill mix of the player/team combination, that is not necessarily the case for this component. If it were, interceptions and fumbles would not be loading well. All six of these variables though do measure the aggressiveness of the QB. The more attempts made, there will subsequently be more interceptions and fumbles.\n\nPC2 - Definition: Offensive Efficiency (Long range effeciency): Attributes: !GamesStarted, Targets, Receptions, ReceivingYards, ReceivingTDs\n\nPC2 can be attributed to total overall offensive effeciency given that we load extremely high for targets, receptions, receiving yards, and receiving touchdowns. We can make the argument that players typically defined for this category would be wide receivers, and pc2 is measuring yardage efficiency. Efficiency is important in this context given that we also load high to targets, and even though it is not an inclusion criteria in fantasy scoring, it speaks to the aggressiveness on the offensive side.\n\nPC3 - Definition:Driving Efficiency (Short range effeciency): Attributes: RushingAttempts, RushingYards, RushingTDs\n\nPC3 would be attributed to mainly RBs and TEs that are elite drivers. They load high for rushing stats. These players would also generally be weaker in long range category. They load below zero for variables in the receiving category such as receivingtds, yardsperreception (so not cathing balls on long distance plays well). Although using the conservative .7 threshold, these would not be considered as loaded for these categories.\n\n\n\n# A tibble: 82 × 4\n    Rank PositionRank Player          FantasyPointsPPR\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1     1            1 Patrick Mahomes             417.\n 2     2            2 Josh Allen                  396.\n 3     3            3 Jalen Hurts                 378 \n 4     7            4 Joe Burrow                  351.\n 5    13            5 Geno Smith                  304.\n 6    17            6 Justin Fields               296 \n 7    18            7 Trevor Lawrence             296.\n 8    19            8 Kirk Cousins                292.\n 9    20            9 Daniel Jones                289 \n10    21           10 Jared Goff                  284.\n# ℹ 72 more rows\n\n\n# A tibble: 82 × 5\n    Rank PositionRank Player          FantasyPointsPPR   pc1\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1     1            1 Patrick Mahomes             417. 6180.\n 2    24           11 Justin Herbert              281. 5761.\n 3    25           12 Tom Brady                   272. 5760.\n 4    19            8 Kirk Cousins                292. 5477.\n 5     7            4 Joe Burrow                  351. 5365.\n 6    21           10 Jared Goff                  284. 5270.\n 7    13            5 Geno Smith                  304. 5126.\n 8     2            2 Josh Allen                  396. 5095.\n 9    18            7 Trevor Lawrence             296. 4959.\n10    36           13 Aaron Rodgers               239. 4479.\n# ℹ 72 more rows\n\n\n# A tibble: 82 × 5\n    Rank PositionRank Player          FantasyPointsPPR   pc1\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt; &lt;dbl&gt;\n 1    25           12 Tom Brady                   272.  5.51\n 2    19            8 Kirk Cousins                292.  5.35\n 3     2            2 Josh Allen                  396.  5.12\n 4    24           11 Justin Herbert              281.  5.11\n 5     1            1 Patrick Mahomes             417.  5.03\n 6    18            7 Trevor Lawrence             296.  5.02\n 7    13            5 Geno Smith                  304.  5.01\n 8     7            4 Joe Burrow                  351.  4.94\n 9    36           13 Aaron Rodgers               239.  4.92\n10    21           10 Jared Goff                  284.  4.72\n# ℹ 72 more rows\n\n\nPCA Scores: Include all variables in the PCA computation, potentially providing a more comprehensive score but with added complexity. High-Loading Variables: Offers a simpler approach but might miss some nuanced information captured by less significant variables."
  },
  {
    "objectID": "fantasyfootball.html",
    "href": "fantasyfootball.html",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "",
    "text": "Around this time every year, if you’re like me, you’ve already started to mentally prepare for 80% of your conversations to be on the topic of football. For those individuals that would not classify themselves as ffanatics, it’s probably annoying - it’s annoying for all of us. Pretty consistently there will behavior from fans that range from DMs to athletes, verbal assaults to close friends, damaged property, and public humiliation. Who realistically has the mental endurance to only discuss a single topic over an extended period of time for something they’re not physically involved in? It’s those with passion, and although I am not passionate about football (nor justify the aforementioned behavior), it will be my first of hopefully many Fantasy Football leagues. If there’s one thing I am passionate about, it’s tilting the odds in my favor. Usually that’s in the form of taking a creative line on the felt on a 2/5 reg by extracting max value with 67s on a low connected board in a 4! pot as the preflop aggressor. Balance and discipline is the name of the game though, and who are we if we don’t apply the same approach to all areas in life…"
  },
  {
    "objectID": "fantasyfootball.html#data-exploration",
    "href": "fantasyfootball.html#data-exploration",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe data was sub set for the 2022 season and then skimmed for to review distributions, counts, and other elements within the data set.\n\n\n\nData summary\n\n\nName\nfantasy\n\n\nNumber of rows\n575\n\n\nNumber of columns\n43\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nfactor\n2\n\n\nnumeric\n39\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nPlayer\n0\n1\n8\n24\n0\n575\n0\n\n\nPlayerID\n0\n1\n8\n8\n0\n575\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nTeam\n0\n1\nFALSE\n34\n2TM: 28, ARI: 20, DEN: 20, LAC: 20\n\n\nFantasyPosition\n0\n1\nFALSE\n4\nWR: 218, RB: 162, TE: 113, QB: 82\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nRank\n0\n1.00\n289.17\n166.96\n1.00\n144.50\n290.00\n433.50\n577.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n26.19\n3.23\n21.00\n24.00\n26.00\n28.00\n45.00\n▇▇▁▁▁\n\n\nGames\n0\n1.00\n11.60\n5.16\n1.00\n8.00\n13.00\n16.00\n17.00\n▂▂▂▂▇\n\n\nGamesStarted\n0\n1.00\n5.56\n5.81\n0.00\n0.00\n3.00\n11.00\n17.00\n▇▂▂▂▂\n\n\nCompletions\n0\n1.00\n19.98\n72.11\n0.00\n0.00\n0.00\n0.00\n490.00\n▇▁▁▁▁\n\n\nAttempts\n0\n1.00\n31.08\n110.35\n0.00\n0.00\n0.00\n0.00\n733.00\n▇▁▁▁▁\n\n\nPassingYards\n0\n1.00\n219.00\n789.12\n0.00\n0.00\n0.00\n0.00\n5250.00\n▇▁▁▁▁\n\n\nPassingTDs\n0\n1.00\n1.30\n5.04\n0.00\n0.00\n0.00\n0.00\n41.00\n▇▁▁▁▁\n\n\nInterceptions\n0\n1.00\n0.72\n2.40\n0.00\n0.00\n0.00\n0.00\n15.00\n▇▁▁▁▁\n\n\nRushingAttempts\n0\n1.00\n25.65\n56.65\n0.00\n0.00\n2.00\n17.00\n349.00\n▇▁▁▁▁\n\n\nRushingYards\n0\n1.00\n114.51\n261.73\n-15.00\n0.00\n5.00\n73.50\n1653.00\n▇▁▁▁▁\n\n\nYardsPerAttempt\n0\n1.00\n2.62\n3.69\n-7.50\n0.00\n1.75\n4.46\n40.00\n▇▇▁▁▁\n\n\nRushingTDs\n0\n1.00\n0.85\n2.14\n0.00\n0.00\n0.00\n1.00\n17.00\n▇▁▁▁▁\n\n\nTargets\n0\n1.00\n29.95\n37.05\n0.00\n3.00\n14.00\n44.00\n184.00\n▇▂▁▁▁\n\n\nReceptions\n0\n1.00\n20.11\n24.93\n0.00\n2.00\n9.00\n30.00\n128.00\n▇▂▁▁▁\n\n\nReceivingYards\n0\n1.00\n220.03\n304.02\n-10.00\n11.00\n96.00\n313.50\n1809.00\n▇▂▁▁▁\n\n\nYardsPerReception\n0\n1.00\n8.67\n6.23\n-6.00\n5.00\n8.90\n12.11\n42.00\n▃▇▂▁▁\n\n\nReceivingTDs\n0\n1.00\n1.30\n2.12\n0.00\n0.00\n0.00\n2.00\n14.00\n▇▁▁▁▁\n\n\nFumbles\n0\n1.00\n1.04\n1.98\n0.00\n0.00\n0.00\n1.00\n16.00\n▇▁▁▁▁\n\n\nFumblesLost\n0\n1.00\n0.48\n0.98\n0.00\n0.00\n0.00\n1.00\n9.00\n▇▁▁▁▁\n\n\nFantasyPointsPPR\n0\n1.00\n78.41\n85.43\n-2.90\n11.50\n43.40\n116.35\n417.40\n▇▂▂▁▁\n\n\nPositionRank\n0\n1.00\n81.82\n55.22\n1.00\n37.00\n73.00\n118.50\n218.00\n▇▇▅▃▂\n\n\nYear\n0\n1.00\n2022.00\n0.00\n2022.00\n2022.00\n2022.00\n2022.00\n2022.00\n▁▁▇▁▁\n\n\nADP\n429\n0.25\n71.90\n42.07\n1.30\n36.42\n70.65\n107.15\n153.80\n▇▇▇▇▅\n\n\nCompletionsPerAttempt\n0\n1.00\n0.10\n0.24\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nTDsPerAttempt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nInterceptionsPerAttempt\n0\n1.00\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n1.00\n▇▁▁▁▁\n\n\nTDsPerReception\n0\n1.00\n0.05\n0.11\n0.00\n0.00\n0.00\n0.07\n1.00\n▇▁▁▁▁\n\n\nFumblesLostPerFumble\n0\n1.00\n0.20\n0.36\n0.00\n0.00\n0.00\n0.25\n1.00\n▇▁▁▁▂\n\n\nscoring_PassingYards\n0\n1.00\n8.76\n31.56\n0.00\n0.00\n0.00\n0.00\n210.00\n▇▁▁▁▁\n\n\nscoring_PassingTDs\n0\n1.00\n5.18\n20.16\n0.00\n0.00\n0.00\n0.00\n164.00\n▇▁▁▁▁\n\n\nscoring_Interceptions\n0\n1.00\n-1.44\n4.80\n-30.00\n0.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\nscoring_RushingYards\n0\n1.00\n1.15\n2.62\n-0.15\n0.00\n0.05\n0.74\n16.53\n▇▁▁▁▁\n\n\nscoring_RushingTDs\n0\n1.00\n5.08\n12.82\n0.00\n0.00\n0.00\n6.00\n102.00\n▇▁▁▁▁\n\n\nscoring_Receptions\n0\n1.00\n20.11\n24.93\n0.00\n2.00\n9.00\n30.00\n128.00\n▇▂▁▁▁\n\n\nscoring_ReceivingYards\n0\n1.00\n22.00\n30.40\n-1.00\n1.10\n9.60\n31.35\n180.90\n▇▂▁▁▁\n\n\nscoring_ReceivingTDs\n0\n1.00\n7.79\n12.72\n0.00\n0.00\n0.00\n12.00\n84.00\n▇▁▁▁▁\n\n\nscoring_Fumbles\n0\n1.00\n-2.09\n3.96\n-32.00\n-2.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\nscoring_FumblesLost\n0\n1.00\n-0.96\n1.96\n-18.00\n-2.00\n0.00\n0.00\n0.00\n▁▁▁▁▇\n\n\n\n\n\nImportant categorical variables to note outside of the Team and Player is the Fantasy Position. These are QBs, WRs, TEs, and RBs. All defensive positions are not scoped and have been explicitly removed from the data set. The objective of this project is to create a fantasy team that has the highest likelihood of obtaining the most Fantasy Points (PPR) for the upcoming season based on prior seasons. The next logical question becomes, does this likelihood vary by team? By position? Intuitively, the likelihood will vary simply based on how many of these positions are in game at a given time. A table of each position shows the distribution of each position in a given NFL season.\n\n\n FantasyPosition   n   percent\n              WR 218 0.3791304\n              RB 162 0.2817391\n              TE 113 0.1965217\n              QB  82 0.1426087\n\n\nA majority of NFL players in 2022 for fantasy purposes were wide receivers (38%) while the least common position were quarterbacks (14%). For the continuous variables, the important things to note are the means, standard deviations, their counts and distribution via the histograms, and any missing values. In-game stats such as Yards Per Attmept, Yards per Reception, and additional variables (below ADP) are missing for a lot of players as expected. The missing values are likely due to a number of factors such as the position of the player and the number of games each played. The range of games played by each player vary from none to 17, representative of 17 total games in the regular season. The descriptive summary for Fantasy Points (PPR) is:\n\n\n   vars   n  mean    sd median trimmed   mad  min   max range skew kurtosis\nX1    1 575 78.41 85.43   43.4      64 57.08 -2.9 417.4 420.3 1.38      1.4\n     se\nX1 3.56\n\n\nWith mean = 78.4 (sd = 85.29), it is obvious that there is high variability. The histogram of PPR shows where how this variability is distributed.\n\n\n\n\n\n\n\n\n\nDepending on the fantasy league, PPR scoring will be different in the sense that there is typically a PPR threshold per game for a player in order for their PPR to be recorded for that given week. If the player is below the baseline, the PPR may be zero. Interestingly enough, the distribution of PPR in 2022 is a log-normal right skew distribution. The observed PPR for most players were approximately zero. This is logical, granted the extreme difficulty of being a top NFL performer. Most players are benched, aren’t playing games, nor are starters. Evidently, most of the continuous variables will follow the same trend with observed frequencies ~0 and relatively a fewer number of players scoring the most for each variable. There are three notable exceptions:\n\n\n\n\n\n\n\n\n\n\nAverage Draft Position - Uniform Distribution: For players that have this data available, ADP is uniformly distributed across most players. This implies that the parent population drafts players evenly across the board and there isn’t a strong concentration of players being picked predominantly. To reiterate, this is specifically for players that have ADP data available, which may be a combination of the most popular or the best players.\nFumbles Lost per Fumble - Bimodal distribution: Frequency of Fumbles Lost per Fumble have peaks at both zero and one, revealing that most players either fumble and lose possession of the ball, resulting in a turnover, or don’t fumble at all.\nYards Per Reception - Normal distribution: Players average 10 Yards Per Reception with a majority of players within the range of two standard deviations from the mean. Although there is a slight right skew, this is one of the few variables that follows a Gaussian distribution.\n\nThe first thing we want to understand is what position, if any, we should be more inclined to draft first, and their likelihood of obtaining the most fantasy points. I started by seeing what the public likes to draft first. In 2022, the public drafted quarterbacks around 86 times on average, the highest of the four positions, with tight ends at a close second of 84 times on average. Running backs and wide receivers then followed. Acknowledging that ADP data is not available for all positions, I wanted to better understand if quarterbacks are the biggest factor in regards to PPR. A comparison of ADP and PPR by position begins to paint the picture. On average, quarterbacks had 105 fantasy points in 22, followed by wide receivers, running backs, then tight ends.\n\n\n# A tibble: 4 × 3\n  FantasyPosition AverageDraftPosition AveragePPR\n  &lt;fct&gt;                          &lt;dbl&gt;      &lt;dbl&gt;\n1 QB                              85.8      105. \n2 WR                              73.5       81.6\n3 RB                              61.5       75.7\n4 TE                              83.9       56.5\n\n\nThis seems to be consistent at a higher level when the top three teams for the 2022 regular season are compared.\n\n\n`summarise()` has grouped output by 'Team'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nHowever, properly accounting for total number of games played by player, yields different results. Evidently, quarterbacks are disproportionately the most efficient in regard to fantasy points on a per game basis at .164 per game. Running backs, tight ends, and receivers consecutively follow but are significantly behind. However, fantasy points between those three positions vary by less than 5%. The logical question becomes what are the underlying causes of this variation? One possibility is that there are typically more wide receivers on an offensive play than any other position. Another possibility can be due to the extreme routes and distances wide receivers run, making them injury prone and therefore less efficient.\n\n\n# A tibble: 4 × 2\n  FantasyPosition PPRperGame\n  &lt;fct&gt;                &lt;dbl&gt;\n1 QB                  0.164 \n2 RB                  0.0393\n3 TE                  0.0372\n4 WR                  0.0316"
  },
  {
    "objectID": "fantasyfootball.html#analysis-svd-pca",
    "href": "fantasyfootball.html#analysis-svd-pca",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Analysis SVD PCA",
    "text": "Analysis SVD PCA\nEigenvalue decomposition is an unsupervised machine learning method typically used for dimensionality reduction on mostly unlabeled data. The same approach is used here, with the intent to reduce the data to components that measure performance in some aspect. The best way to think about this is in the form of a recipe. A cookbook’s recipe for chicken cordon bleu will have elaborate concoctions and mixes of different food. When applied in this context, it would reduce the recipe to its core components, 3/4 chicken, 1/8 cheese, 1/8 ham let’s say. It then becomes much easier to make chicken cordon bleu while keeping most of the taste. While an oversimplification, the approach is essentially the same, with the goal of maximizing the amount of underlying variation using linear combinations of variables. At its core, there is some latent underlying variable(s) that combinations of these variables measure. What those underlying variables measure and its relevancy is on us to define. These are the principal components. The principal components are made up of the original variables, and how much that variable contributes to the underlying variable (ie principal component) are the eigen vectors or loadings. Loadings can be positive (greatly contributes) or negative (adversely contributes). How well the variables load help define what that new underlying variable is. To define the inclusion criteria, any variable that loads +/- .7 will be considered as loading well and those variables alone will be what is used to define the underlying variable/component. This empirical threshold is a very conservative approach.\nEigen vectors describe a mathematical phenomena such that\n\\[ A * v =  λ * v \\]\nwhere A is a square matrix, v is an eigen vector, and λ is a scalar (numerical value) and the associated eigen value of vector v. In this application, matrix A is correlation matrix of the original data. This mechanism works because linear transformations are applied to the data meaning the data does not inherently change. The proportions of all variables and the direction in which they move remain the same. The data gets centered at the origin after scaling, and a best fitting line is calculated that goes through the origin and maximizes the variance in the data. The algorithm does this by fitting a random line through the data, projecting the points onto the line, and calculating the largest sum of squared difference. The yielded line of best fit is the eigen vector for the principal component and the slope is the eigen value.\n\n\n\n\n\n\n\n\n\nA parallel test was used to measure the number of components to obtain. The test performs the same decomposition on simulated data of the same size and graphs the results. Where the simulated and actual data intersect is the cutoff for the number of components to obtain. The results of the test suggest three components. The y-axis plots the eigenvalues which is the total variation explained by each component. In a simpler sense, it can be thought of as the number of original variables accounted for in that component (hence the horizontal line separating values less than one). Three principal components were obtained, acknowledging that principal component one (PC1) should account for approximately six variables, PC2 around 5, and PC3 around 3. The other scree plot better highlights the components as a percentage of total variability explained. Keep in mind that PC1 only accounts for 30% of the total variability and the first three components cumulatively account for 52% of total variability. It’s likely that the post-hoc tests described in the methods section will not be sufficient for our goal since there is still half of the total variation not accounted for in these components.\nScores are calculated for each individual player. Depending on how the components are defined, players can be ranked in ascending order."
  },
  {
    "objectID": "fantasyfootball.html#principal-components",
    "href": "fantasyfootball.html#principal-components",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Principal Components",
    "text": "Principal Components\nrotation: maximum variance\nPC1 Definition: High-Volume Performing QBs\nAttributes: Completions, Attempts, Passing Yards, Passing TDs, Interceptions, Fumbles\n\n\nPC1 would be attributed to high-volume QBs as they load extremely well for the above categories. The first inclination was to attribute PC1 to high performing quarter backs, however, that statement alone would be unjustifiable considering that interceptions and fumbles load extremely well to this component. High-volume quarter backs would be a more fitting description. These quarter backs are performing extremely well in some regard since it loads high for completions, passing yards, and passing touch downs. We can reason that these quarter backs are also able to consistently get the ball off of their hands. High-volume quarter backs will also load high to interceptions and fumbles. The more throws and attempts made, the more likely that fumbles and interceptions will occur.\n\nPC2 Definition: Offensive Long Range Efficiency\nAttributes: Games Started, Targets, Receptions, Receiving Yards, Receiving TDs\n\n\nPC2 can be attributed to total overall offensive efficiency given that we load extremely high for targets, receptions, receiving yards, and receiving touchdowns. Players typically defined for this category would be wide receivers, and pc2 is measuring yardage efficiency. Efficiency is important in this context given that we also load high to targets, and even though it is not an inclusion criteria in fantasy scoring, it speaks to the aggressiveness on the offensive side. The key distinction to make here is that this describes the overall long range efficiency only since rushing yards and touch downs are not accounted for in this component. Additionally, overall long range efficiency is justified since this must be a combination of quarter backs and the offensive line. Wide receivers, running backs, and tight ends will generally only score more touch downs and have more yards with a good quarter back.\n\nPC3 Definition: Offensive Driving Efficiency\nAttributes: Rushing Attempts, Rushing Yards, Rushing TDs\n\n\nPC3 would be attributed to mainly RBs and TEs that are elite drivers since they load high for rushing stats.\n\nIdeally, we’d want to load players that load high for all three categories. Considering the nature of football, depending on player’s primary position, they will naturally perform better in certain stats or categories over others. In this case, there are multiple approaches to account for this. Actually, along every step I find there are ways that our paths diverge, but more on that later. One approach is to use only principal component one and players/scores that load high for that component to pick our quarter back. Principal components two and three would then be used for all other positions. I started with that approach but here comes the other divergence - how I choose the calculate the scores. There are two options under consideration.\n\nInclude all variables in the principal component computation, with the benefit of providing a more comprehensive score but the drawback of added complexity.\nInclude only variables in the principal component computations that load high as the score, with the benefit of exclusively calculating how good they are at being good but the drawback of missing nuanced information capture in less significant variables.\n\nI tested the model starting with the second approach. First, I reviewed how the top ten quarter backs performed in 2022 by looking at their overall rank, position rank, the player, and the total fantasy points they had.\n\n\n# A tibble: 10 × 4\n    Rank PositionRank Player          FantasyPointsPPR\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1     1            1 Patrick Mahomes             417.\n 2     2            2 Josh Allen                  396.\n 3     3            3 Jalen Hurts                 378 \n 4     7            4 Joe Burrow                  351.\n 5    13            5 Geno Smith                  304.\n 6    17            6 Justin Fields               296 \n 7    18            7 Trevor Lawrence             296.\n 8    19            8 Kirk Cousins                292.\n 9    20            9 Daniel Jones                289 \n10    21           10 Jared Goff                  284.\n\n\nThen we use the eigen values to calculate the principal component scores for each player, only including variables that loaded high. The players with the highest scores would be the highest performing QBs predicted for the 2022 season. Two things to note. Firstly, the position and overall rank will be the same here since we’ve define our first principal component as attributes of quarter backs only. Secondly, the model’s fantasy points ppr would not be known. The purpose here is to calculate scores and draft in ascending order. We can, however, calculate the difference in fantasy points had we taken the models’ picks. In the table, the model’s fantasy points are the same as actual fantasy points to make this calculation easier. The results are shown below.\nHere’s the difference in the models vs the actual performance for the top 10 quarter backs of the 2022 regular season.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nPositionRank\nPlayer\nFantasyPointsPPR\nModel_PositionRank\nPlayer\nModel_FantasyPointsPPR\nModel_Score\n\n\n\n\n1\n1\nPatrick Mahomes\n417.4\n1\nPatrick Mahomes\n417.4\n6180.013\n\n\n2\n2\nJosh Allen\n395.5\n2\nJustin Herbert\n281.3\n5761.349\n\n\n3\n3\nJalen Hurts\n378.0\n3\nTom Brady\n271.7\n5760.128\n\n\n7\n4\nJoe Burrow\n350.7\n4\nKirk Cousins\n291.6\n5476.825\n\n\n13\n5\nGeno Smith\n303.9\n5\nJoe Burrow\n350.7\n5364.627\n\n\n17\n6\nJustin Fields\n296.0\n6\nJared Goff\n284.3\n5269.900\n\n\n18\n7\nTrevor Lawrence\n295.6\n7\nGeno Smith\n303.9\n5126.406\n\n\n19\n8\nKirk Cousins\n291.6\n8\nJosh Allen\n395.5\n5095.194\n\n\n20\n9\nDaniel Jones\n289.0\n9\nTrevor Lawrence\n295.6\n4958.574\n\n\n21\n10\nJared Goff\n284.3\n10\nAaron Rodgers\n239.2\n4479.419"
  },
  {
    "objectID": "fantasyfootball.html#principal-components---in-game-stats",
    "href": "fantasyfootball.html#principal-components---in-game-stats",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Principal Components - In-Game Stats",
    "text": "Principal Components - In-Game Stats\nrotation: maximum variance\nPC1 Definition: High-Volume Performing QBs\nAttributes: Completions, Attempts, Passing Yards, Passing TDs, Interceptions, Fumbles\n\n\nPC1 would be attributed to high-volume QBs as they load extremely well for the above categories. The first inclination was to attribute PC1 to high performing quarter backs, however, that statement alone would be unjustifiable considering that interceptions and fumbles load extremely well to this component. High-volume quarter backs would be a more fitting description. These quarter backs are performing extremely well in some regard since it loads high for completions, passing yards, and passing touch downs. We can reason that these quarter backs are also able to consistently get the ball off of their hands. High-volume quarter backs will also load high to interceptions and fumbles. The more throws and attempts made, the more likely that fumbles and interceptions will occur.\n\nPC2 Definition: Offensive Long Range Efficiency\nAttributes: Games Started, Targets, Receptions, Receiving Yards, Receiving TDs\n\n\nPC2 can be attributed to total overall offensive efficiency given that we load extremely high for targets, receptions, receiving yards, and receiving touchdowns. Players typically defined for this category would be wide receivers, and pc2 is measuring yardage efficiency. Efficiency is important in this context given that we also load high to targets, and even though it is not an inclusion criteria in fantasy scoring, it speaks to the aggressiveness on the offensive side. The key distinction to make here is that this describes the overall long range efficiency only, since rushing yards and touch downs are not accounted for in this component. Additionally, overall long range efficiency is justified since this must be a combination of quarter backs and the offensive line. Wide receivers, running backs, and tight ends will generally only score more touch downs and have more yards with a good quarter back.\n\nPC3 Definition: Offensive Driving Efficiency\nAttributes: Rushing Attempts, Rushing Yards, Rushing TDs\n\n\nPC3 would be attributed to mainly RBs and TEs that are elite drivers since they load high for rushing stats.\n\nIdeally, we’d want to load players that load high for all three categories. Considering the nature of football, depending on player’s primary position, they will naturally perform better in certain stats or categories over others. In this case, there are multiple approaches to account for this. Actually, along every step I find there are ways that our paths diverge, but more on that later. One approach is to use only principal component one and players/scores that load high for that component to pick our quarter back. Principal components two and three would then be used for all other positions. I started with that approach but here comes the other divergence - how I choose the calculate the scores. There are two options under consideration.\n\nInclude all variables in the principal component computation, with the benefit of providing a more comprehensive score but the drawback of added complexity.\nInclude only variables in the principal component computations that load high as the score, with the benefit of exclusively calculating how good they are at being good but the drawback of missing nuanced information capture in less significant variables.\n\nI tested the model starting with the second approach. First, I reviewed how the top ten quarter backs performed in 2022 by looking at their overall rank, position rank, the player, and the total fantasy points they had.\n\n\n# A tibble: 10 × 4\n    Rank PositionRank Player          FantasyPointsPPR\n   &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                      &lt;dbl&gt;\n 1     1            1 Patrick Mahomes             417.\n 2     2            2 Josh Allen                  396.\n 3     3            3 Jalen Hurts                 378 \n 4     7            4 Joe Burrow                  351.\n 5    13            5 Geno Smith                  304.\n 6    17            6 Justin Fields               296 \n 7    18            7 Trevor Lawrence             296.\n 8    19            8 Kirk Cousins                292.\n 9    20            9 Daniel Jones                289 \n10    21           10 Jared Goff                  284.\n\n\nThen we use the eigen values to calculate the principal component scores for each player, only including variables that loaded high. The players with the highest scores would be the highest performing QBs predicted for the 2022 season. Two things to note. Firstly, the position and overall rank will be the same here since we’ve define our first principal component as attributes of quarter backs only. Secondly, the model’s fantasy points ppr would not be known. The purpose here is to calculate scores and draft in ascending order. We can, however, calculate the difference in fantasy points had we taken the models’ picks. In the table, the model’s fantasy points are the same as actual fantasy points to make this calculation easier. The results are shown below.\nMethods of measuring model performance:\n\nPercentage of total players the model accurately selects. If we were selecting quarter backs, we would use the first principal component scores to obtain the top 10 quarter backs in ascending order. We would then compare the results against the actual top 10 quarter backs for that season. In these results, 70% (7 out of 10 QBs) were accurately selected as being in the top 10 for total fantasy points.\nDifference in total fantasy points of the top 20 players. The top 20 players for each position are isolated using their associated principal component scores. The sum of the total fantasy points for the top 20 players are then subtracted from the what the actual total fantasy points for players in the top 20 in each position had to obtain the delta.\nAbsolute difference in position rank by player. Each player will have the net difference in their position rank between the model and their actual rank for that season. In the above table, for example, the model selects Justin Herbert as #2 QB for fantasy but was actually #11 after the regular season, making the net -11.\n\nNow using the model’s picks for the top ten quarter backs, 70% of those selected in the top 10 were actually in the top ten during the 2022 regular season. The total fantasy points for the quarter backs picked by the model were 3131.2. The total fantasy points the top ten quarter backs actually had was 3302 which means the model was off by 5% in regard to quarter back selection. The difference for rank by position are shown below. Players with a negative delta are those that were ranked higher in the model but came in lower after the season.\n\n\n# A tibble: 10 × 2\n   Model_Player    PositionRankDelta\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 Patrick Mahomes                 0\n 2 Justin Herbert                 -9\n 3 Tom Brady                      -9\n 4 Kirk Cousins                   -4\n 5 Joe Burrow                      1\n 6 Jared Goff                     -4\n 7 Geno Smith                      2\n 8 Josh Allen                      6\n 9 Trevor Lawrence                 2\n10 Aaron Rodgers                  -3"
  },
  {
    "objectID": "fantasyfootball.html#principal-components---fantasy-weights",
    "href": "fantasyfootball.html#principal-components---fantasy-weights",
    "title": "Singular Value Decomposition - Enhanced Principal Components to Optimize Fantasy Football Teams",
    "section": "Principal Components - Fantasy Weights",
    "text": "Principal Components - Fantasy Weights\nWhile the results are seemingly great, this was when I recalled that these principal components only account for half of the total variation. Considering that the goal of singular value decomposition via principal component is to maximize the total variation in the data, I had to think we can do much better. The first principal component which was attributed to quarter backs load high for interceptions and fumbles. While it is justifiable to reason that high-volume quarter backs will naturally intercept and fumble more often solely as a function of volume, would the best quarter backs really load high for those? To better understand this, the exact same process above was done to calculate new component scores, this time only including variables that are used for fantasy scoring. I first created these additional variables by multiplying them by my league’s point system. For example, rushing touch downs were multiplied by six and became the new variable used for the svd. Those variables were passing yards, passing touchdowns, interceptions, rushing yards, rushing touchdowns, receptions, receiving yards, receiving touch downs, fumbles, and fumbles lost.\n\nResults\nThe parallel test suggested three components were sufficient to explain the maximum variation in the data.\n\n\n\n\n\n\n\n\n\nThe first three components alone account for 88.1% of the total variation within the data set, much better than the 50% obtained previously. The mean item complexity = 1.1. This means that each individual variable included in the principal components only load significantly on one component. This is the more ideal scenario since it makes defining the components much easier. Previously we had a mean item complexity of 1.5, meaning that half of the variables on average load significantly to two components. The first three components are then defined using a loading threshold of .7.\n\n\n\n\n\n\n\n\n\n\n\nDimension Definitions\nrotation: maximum variance\nPC1 Definition: Low Performing QBs\nAttributes: (-) Passing Yards, (-) Passing Touchdowns, (+) Fumbles, (+) Interceptions\n\n\nPC1 would be attributed to quite literally the least performing quarter backs. The significantly negative loadings for passing yards and touch downs mean that quarter backs that load high to this component are unable to score touch downs. Additionally, they load extremely high for fumbles and interceptions, a confirmation of their under performance in relevant categories. PC1 would only be attributed to quarter backs since these in-game stats are generally relevant to them alone.\n\nPC2 Definition: High-Performing Distance Efficiency\nAttributes: + Receiving Touchdowns, + Receiving Yards, + Receptions\n\n\nPC2 can be attributed to total overall offensive efficiency given that we load extremely high for receiving touchdowns, receiving yards, and receptions. Players that load high to this category are likely wide receivers since wide receivers are more used for long range plays. The key distinction to make here is that this describes the overall long range efficiency only, since rushing yards and rushing touch downs are not accounted for in this component. Additionally, overall long range efficiency would better describe this component, since this must be a combination of quarter backs and the offensive line. Wide receivers, running backs, and tight ends will generally only score more touch downs and have more yards with a better quarter back.\n\nPC3 Definition: High-Performing Driving Efficiency\nAttributes: + Rushing Touchdowns, + Rushing Yards\n\n\nPC3 would be attributed to overall driving efficiency in the same fashion. Likewise, this is also a combination of the offense line and the quarterback, considering that high performing drivers will still be unable to score touch downs in some fashion if their quarter back cannot perform. I would expect running backs and tight ends to load high to this category.\n\nThe goal is to isolate players in these areas.\n\n\n\n\n\n\n\n\n\nThe coordinate plane shows the first component on the x axis and the second on the y axis. The scoring_ and associated arrows are the eigen vectors on this principal component space. An increase on the x axis, or the first principal component, we increase in under performance. We would want to obtain players that negative contribute to this component ie quadrant three. In the same fashion, an increase on the y axis means an increase in long range efficiency. This can only show the first two components. Based on the above, the we’d use the first component for quarter backs, the second for wide receivers, and the third for tight ends and running backs. However, since WRs, TEs, and RBs are much more similar in position (which the model concurs via the boxed region in quadrant I) than QBs, those three positions were included and ranked for PC2 and PC3. The allows us to more effectively see the primary position and players the model decides to pick for each category. In summary, principal component one was used for quarter backs, principal components two and three were used for all other positions at first. An overall score was then calculated using PC2 and PC3 only. Players with the highest overall scores would load significantly well to PC2 and PC3. These players would be both the best of the best in both long-range and driving efficiency. This allows us to see what positions the model picks for long-range (PC2) and driving (PC3) efficiency. Note that this tells us what position would be the best at both but does not tell us if they are the best at both.\n\n\n# A tibble: 3 × 2\n  FantasyPosition       n\n  &lt;fct&gt;             &lt;dbl&gt;\n1 RB               0.455 \n2 WR               0.0738\n3 TE              -0.287 \n\n\nWe see that running backs and wide receivers on average have positive scores for both, meaning that they contribute positively to the second and third components (long and driving efficiency). Surprisingly enough, tight tends on average are negative in both regards. The model rarely picks tight ends at all across all three of these dimensions. The model tries to pick the best overall QBs via the lowest component scores for PC1, the best wide receivers for PC2, and the best RBs for PC3. That processed was followed with plans of addressing the ‘tight end’ problem afterwards. The model’s results for QBs, WRs, and RBs are below.\n\n\nModel Picks\n\nPC1 - QBs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nPositionRank\nPlayer\nFantasyPointsPPR\nActual_PositionRank\nModel_PositionRank\nModel_Player\nModel_FantasyPointsPPR\nModel_PCscore\n\n\n\n\n1\n1\nPatrick Mahomes\n417.4\n1\n1\nPatrick Mahomes\n417.4\n-385.8120\n\n\n2\n2\nJosh Allen\n395.5\n2\n2\nJosh Allen\n395.5\n-350.7159\n\n\n3\n3\nJalen Hurts\n378.0\n4\n3\nJoe Burrow\n350.7\n-339.5820\n\n\n7\n4\nJoe Burrow\n350.7\n8\n4\nKirk Cousins\n291.6\n-325.2073\n\n\n13\n5\nGeno Smith\n303.9\n5\n5\nGeno Smith\n303.9\n-316.3577\n\n\n17\n6\nJustin Fields\n296.0\n11\n6\nJustin Herbert\n281.3\n-311.7454\n\n\n18\n7\nTrevor Lawrence\n295.6\n10\n7\nJared Goff\n284.3\n-309.3991\n\n\n19\n8\nKirk Cousins\n291.6\n12\n8\nTom Brady\n271.7\n-304.4806\n\n\n20\n9\nDaniel Jones\n289.0\n7\n9\nTrevor Lawrence\n295.6\n-299.4071\n\n\n21\n10\nJared Goff\n284.3\n13\n10\nAaron Rodgers\n239.2\n-280.7368\n\n\n\n\n\nThe results are nearly identical to the previous method (on the original variables). Model accurately picks seven of the top ten players.\n\n\nPC2 - WR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nPositionRank\nPlayer\nFantasyPointsPPR\nActual_PositionRank\nModel_PositionRank\nModel_Player\nModel_FantasyPointsPPR\nModel_PCscore\n\n\n\n\n5\n1\nJustin Jefferson\n368.7\n1\n1\nJustin Jefferson\n368.7\n343.4864\n\n\n8\n2\nTyreek Hill\n347.2\n3\n2\nDavante Adams\n335.5\n321.2936\n\n\n9\n3\nDavante Adams\n335.5\n2\n3\nTyreek Hill\n347.2\n319.6640\n\n\n11\n4\nStefon Diggs\n316.6\n4\n4\nStefon Diggs\n316.6\n303.8524\n\n\n15\n5\nCeeDee Lamb\n301.6\n1\n5\nTravis Kelce\n316.3\n302.4248\n\n\n16\n6\nA.J. Brown\n299.6\n6\n6\nA.J. Brown\n299.6\n291.2316\n\n\n26\n7\nAmon-Ra St. Brown\n267.6\n5\n7\nCeeDee Lamb\n301.6\n284.9864\n\n\n27\n8\nJaylen Waddle\n259.2\n8\n8\nJaylen Waddle\n259.2\n248.4996\n\n\n28\n9\nDeVonta Smith\n254.6\n7\n9\nAmon-Ra St. Brown\n267.6\n248.0896\n\n\n32\n10\nAmari Cooper\n246.0\n9\n10\nDeVonta Smith\n254.6\n246.5056\n\n\n\n\n\n\n\nPC3 - RB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRank\nPositionRank\nPlayer\nFantasyPointsPPR\nActual_PositionRank\nModel_PositionRank\nModel_Player\nModel_FantasyPointsPPR\nModel_PCscore\n\n\n\n\n5\n1\nJustin Jefferson\n368.7\n1\n1\nJustin Jefferson\n368.7\n343.4864\n\n\n8\n2\nTyreek Hill\n347.2\n3\n2\nDavante Adams\n335.5\n321.2936\n\n\n9\n3\nDavante Adams\n335.5\n2\n3\nTyreek Hill\n347.2\n319.6640\n\n\n11\n4\nStefon Diggs\n316.6\n4\n4\nStefon Diggs\n316.6\n303.8524\n\n\n15\n5\nCeeDee Lamb\n301.6\n1\n5\nTravis Kelce\n316.3\n302.4248\n\n\n16\n6\nA.J. Brown\n299.6\n6\n6\nA.J. Brown\n299.6\n291.2316\n\n\n26\n7\nAmon-Ra St. Brown\n267.6\n5\n7\nCeeDee Lamb\n301.6\n284.9864\n\n\n27\n8\nJaylen Waddle\n259.2\n8\n8\nJaylen Waddle\n259.2\n248.4996\n\n\n28\n9\nDeVonta Smith\n254.6\n7\n9\nAmon-Ra St. Brown\n267.6\n248.0896\n\n\n32\n10\nAmari Cooper\n246.0\n9\n10\nDeVonta Smith\n254.6\n246.5056"
  }
]